services:
  # =============================================
  # Local Language Models (GPU required)
  # =============================================
  qwen-chat:
    build:
      context: ./local_models
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME=Qwen/Qwen1.5-4B-Chat
      - HF_TOKEN=${HF_TOKEN}
    runtime: nvidia
    volumes:
      - ./cache/models:/home/appuser/.cache/huggingface/hub
    networks:
      - ai-challenge-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  mistral-chat:
    build:
      context: ./local_models
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    environment:
      - MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
      - HF_TOKEN=${HF_TOKEN:-}
    runtime: nvidia
    volumes:
      - ./cache/models:/home/appuser/.cache/huggingface/hub
    networks:
      - ai-challenge-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  tinyllama-chat:
    build:
      context: ./local_models
      dockerfile: Dockerfile
    ports:
      - "8002:8000"
    environment:
      - MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - HF_TOKEN=${HF_TOKEN:-}
    runtime: nvidia
    volumes:
      - ./cache/models:/home/appuser/.cache/huggingface/hub
    networks:
      - ai-challenge-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  starcoder-chat:
    build:
      context: ./local_models
      dockerfile: Dockerfile
    ports:
      - "8003:8000"
    environment:
      - MODEL_NAME=TechxGenus/starcoder2-7b-instruct
      - HF_TOKEN=${HF_TOKEN:-}
    runtime: nvidia
    volumes:
      - ./cache/models:/home/appuser/.cache/huggingface/hub
    networks:
      - ai-challenge-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  # =============================================
  # MCP Server (Phase 4)
  # =============================================
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile.mcp
    image: ai-challenge-mcp:day10
    container_name: mcp-server-day10
    ports:
      - "8004:8004"  # HTTP API
    environment:
      - MODEL_TIMEOUT_SECONDS=60
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - QWEN_URL=http://qwen-chat:8000
      - MISTRAL_URL=http://mistral-chat:8000
      - TINYLLAMA_URL=http://tinyllama-chat:8000
      - STARCODER_URL=http://starcoder-chat:8000
    networks:
      - ai-challenge-network
    # Note: Services will wait for models to be ready via retries
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # =============================================
  # Main API (Optional)
  # =============================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-challenge-api
    ports:
      - "8080:8000"
    environment:
      - STORAGE_PATH=/app/data
      - MODEL_NAME=gpt-4
      - MODEL_MAX_TOKENS=4096
      - MODEL_TEMPERATURE=0.7
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
    networks:
      - ai-challenge-network
    depends_on:
      - mcp-server
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

# ============================================= formatting
# Volumes - Models cache is mounted as local directory
# =============================================
# Note: cache/models is mounted as local directory to persist
# across docker prune operations

# =============================================
# Networks
# =============================================
networks:
  ai-challenge-network:
    driver: bridge
    name: ai-challenge-network

