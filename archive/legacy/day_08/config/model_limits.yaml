# Model limits configuration in YAML format.
# This file contains token limits for different models
# with both theoretical and practical profiles.

# Model limits configuration
models:
  starcoder:
    theoretical:
      max_input_tokens: 16384
      max_output_tokens: 2048
      max_total_tokens: 16384
      sliding_window: 4096
      description: "Full architecture support"
    
    practical:
      max_input_tokens: 4096
      max_output_tokens: 1024
      max_total_tokens: 6000
      recommended_input: 3500
      description: "Safe for 8GB GPU with flash-attention"
  
  mistral:
    theoretical:
      max_input_tokens: 32768
      max_output_tokens: 2048
      max_total_tokens: 32768
      sliding_window: 8192
      description: "Full Mistral architecture"
    
    practical:
      max_input_tokens: 8192
      max_output_tokens: 1024
      max_total_tokens: 10000
      recommended_input: 7000
      description: "Conservative for 8GB GPU"
  
  qwen:
    theoretical:
      max_input_tokens: 32768
      max_output_tokens: 2048
      max_total_tokens: 32768
      sliding_window: 8192
      description: "Full Qwen architecture"
    
    practical:
      max_input_tokens: 8192
      max_output_tokens: 1024
      max_total_tokens: 10000
      recommended_input: 7000
      description: "Conservative for 8GB GPU"
  
  tinyllama:
    theoretical:
      max_input_tokens: 2048
      max_output_tokens: 512
      max_total_tokens: 2048
      sliding_window: 1024
      description: "Full TinyLlama architecture"
    
    practical:
      max_input_tokens: 2048
      max_output_tokens: 512
      max_total_tokens: 2048
      recommended_input: 1800
      description: "Already small, no reduction needed"

# Default settings
defaults:
  profile: "practical"
  safety_margin: 0.9
  tokens_per_word_ratio: 1.3

# Metadata
metadata:
  version: "1.0.0"
  last_updated: "2024-01-01"
  description: "Token limits for various LLM models"
