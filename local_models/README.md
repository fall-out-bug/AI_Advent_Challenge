# üè† Local Models - TechxGenus/StarCoder2-7B-Instruct Service Infrastructure

This module provides the infrastructure for running TechxGenus/StarCoder2-7B-Instruct model locally with Docker. It's designed to serve as the backend for the multi-agent system in day_07.

## üéØ Purpose

- **TechxGenus/StarCoder2-7B-Instruct Service**: Local hosting of instruction-tuned StarCoder model for code generation
- **Docker Orchestration**: Automated container management with security best practices
- **API Compatibility**: OpenAI-compatible chat API interface
- **Security**: Non-root user, optimized layers, health checks

## üìÅ Structure

```
local_models/
‚îú‚îÄ‚îÄ chat_api.py                    # FastAPI server for all models
‚îú‚îÄ‚îÄ docker-compose.yml            # Docker Compose configuration
‚îú‚îÄ‚îÄ Dockerfile                    # Optimized Docker image with security
‚îú‚îÄ‚îÄ download_model.py             # Script for model pre-downloading
‚îú‚îÄ‚îÄ download_models.sh            # Management script for downloads
‚îú‚îÄ‚îÄ Dockerfile.download           # Docker image for downloading
‚îú‚îÄ‚îÄ docker-compose.download.yml   # Docker Compose for downloads
‚îú‚îÄ‚îÄ requirements.download.txt     # Minimal dependencies for downloading
‚îú‚îÄ‚îÄ DOWNLOAD_GUIDE.md             # Complete download guide
‚îú‚îÄ‚îÄ .env.example                  # Environment variables template
‚îú‚îÄ‚îÄ .env                         # Environment variables (create from example)
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üöÄ Quick Start

### 1. Pre-download Models (Recommended)

For faster startup and offline usage, pre-download all models:

```bash
# Download all models to cache
./download_models.sh download-all

# Or download specific models
./download_models.sh download-model Qwen/Qwen1.5-4B-Chat
```

üìñ **Detailed guide**: See [DOWNLOAD_GUIDE.md](DOWNLOAD_GUIDE.md) for complete instructions.

### 2. Environment Setup

```bash
# Copy environment template
cp .env.example .env

# Edit .env file with your Hugging Face token
HF_TOKEN=your_huggingface_token_here
```

### 3. Start Chat Services

```bash
# Start all chat services
docker-compose up -d

# Or start specific services
docker-compose up -d qwen-chat mistral-chat tinyllama-chat starcoder-chat
```

### 4. Verify Services

```bash
# Check all services
curl http://localhost:8000/health  # Qwen
curl http://localhost:8001/health  # Mistral
curl http://localhost:8002/health  # TinyLlama
curl http://localhost:8003/health  # StarCoder

# Test chat endpoint
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello"}],"max_tokens":50}'
```

## üîí Docker Security Features

The Dockerfile implements security best practices:

- **Non-root user**: Runs as `appuser` instead of root
- **Optimized layers**: Combined RUN commands to reduce image size
- **Health checks**: Built-in health monitoring
- **Minimal base**: Uses NVIDIA CUDA runtime with minimal dependencies
- **Proper ownership**: Files and directories have correct permissions

## üìä Resource Requirements

- **GPU**: NVIDIA GPU with CUDA support
- **RAM**: 8-16GB recommended for StarCoder-7B
- **Storage**: ~15GB for model cache
- **Port**: 8003 (configurable in docker-compose.yml)

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π

```bash
# –ò–∑ –∫–æ—Ä–Ω—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
cd local_models
docker-compose up -d
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
curl http://localhost:8000/chat  # Qwen
curl http://localhost:8001/chat  # Mistral  
curl http://localhost:8002/chat  # TinyLlama
```

### 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö

```python
# –í –ª—é–±–æ–º –ø—Ä–æ–µ–∫—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
import httpx

async def call_local_model(model_name: str, messages: list):
    port = {"qwen": 8000, "mistral": 8001, "tinyllama": 8002}[model_name]
    url = f"http://localhost:{port}/chat"
    
    async with httpx.AsyncClient() as client:
        response = await client.post(url, json={
            "messages": messages,
            "max_tokens": 500,
            "temperature": 0.7
        })
        return response.json()
```

## ü§ñ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏

| –ú–æ–¥–µ–ª—å | –ü–æ—Ä—Ç | RAM | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|------|-----|----------|
| **Qwen-4B** | 8000 | ~8GB | –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| **Mistral-7B** | 8001 | ~14GB | –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ |
| **TinyLlama-1.1B** | 8002 | ~4GB | –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è, –±—ã—Å—Ç—Ä–∞—è |

## üîß –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### FastAPI –°–µ—Ä–≤–µ—Ä (`chat_api.py`)

- **–ï–¥–∏–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç**: `/chat` –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- **OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–æ–≤
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤
- **–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è**: 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏

### Docker –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è

- **–û–±—â–∏–π –∫—ç—à**: –í—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–±—â–∏–π volume –¥–ª—è HuggingFace –∫—ç—à–∞
- **GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ NVIDIA GPU
- **–ò–∑–æ–ª—è—Ü–∏—è**: –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ

### –§–æ—Ä–º–∞—Ç—ã –ø—Ä–æ–º–ø—Ç–æ–≤

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏:

- **Mistral**: `<s>[INST] <<SYS>>...<</SYS>>...[/INST]`
- **Qwen**: `<|im_start|>system...<|im_end|>...`
- **TinyLlama**: `<|system|>...<|user|>...<|assistant|>`

## üìä API –°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è

### –ó–∞–ø—Ä–æ—Å

```json
POST /chat
{
    "messages": [
        {"role": "system", "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"},
        {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}
    ],
    "max_tokens": 500,
    "temperature": 0.7
}
```

### –û—Ç–≤–µ—Ç

```json
{
    "response": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
    "response_tokens": 5,
    "input_tokens": 12,
    "total_tokens": 17
}
```

## üõ†Ô∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

### –ö–æ–º–∞–Ω–¥—ã Docker Compose

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
docker-compose up -d

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π  
docker-compose down

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
docker-compose logs -f

# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
docker-compose restart qwen-chat
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
docker-compose ps

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
docker stats

# –õ–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
docker-compose logs qwen-chat
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

- `MODEL_NAME`: –ò–º—è –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace
- `HF_TOKEN`: –¢–æ–∫–µ–Ω –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º
- `CUDA_VISIBLE_DEVICES`: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ GPU

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏

1. –î–æ–±–∞–≤–∏—Ç—å —Å–µ—Ä–≤–∏—Å –≤ `docker-compose.yml`:
```yaml
new-model-chat:
  build: .
  ports:
    - "8003:8000"
  environment:
    - MODEL_NAME=your/model-name
```

2. –û–±–Ω–æ–≤–∏—Ç—å –º–∞–ø–ø–∏–Ω–≥ –ø–æ—Ä—Ç–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö:
```python
MODEL_PORTS = {
    "qwen": 8000,
    "mistral": 8001, 
    "tinyllama": 8002,
    "new-model": 8003  # –ù–æ–≤—ã–π –ø–æ—Ä—Ç
}
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

```bash
# –¢–µ—Å—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
for port in 8000 8001 8002; do
  echo "Testing port $port..."
  curl -X POST http://localhost:$port/chat \
    -H "Content-Type: application/json" \
    -d '{"messages":[{"role":"user","content":"Hello"}],"max_tokens":10}'
done
```

### –ë–µ–Ω—á–º–∞—Ä–∫–∏

```python
import asyncio
import time
import httpx

async def benchmark_model(model_name: str, num_requests: int = 10):
    port = {"qwen": 8000, "mistral": 8001, "tinyllama": 8002}[model_name]
    url = f"http://localhost:{port}/chat"
    
    start_time = time.time()
    async with httpx.AsyncClient() as client:
        tasks = []
        for _ in range(num_requests):
            task = client.post(url, json={
                "messages": [{"role": "user", "content": "Test message"}],
                "max_tokens": 50
            })
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
    
    total_time = time.time() - start_time
    avg_time = total_time / num_requests
    
    print(f"{model_name}: {avg_time:.2f}s per request")
    return avg_time
```

## üö® –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU:
```bash
nvidia-smi
```

2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–≤–æ–±–æ–¥–Ω—É—é –ø–∞–º—è—Ç—å:
```bash
free -h
```

3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏:
```bash
docker-compose logs model-name
```

### –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã

1. –£–≤–µ–ª–∏—á–∏—Ç—å `max_tokens` –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
2. –£–º–µ–Ω—å—à–∏—Ç—å `temperature` –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É GPU: `nvidia-smi`

### –û—à–∏–±–∫–∏ –ø–∞–º—è—Ç–∏

1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å (TinyLlama)
2. –£–º–µ–Ω—å—à–∏—Ç—å `max_tokens`
3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

- **–î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏**: TinyLlama (–±—ã—Å—Ç—Ä–æ, –º–∞–ª–æ –ø–∞–º—è—Ç–∏)
- **–î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞**: Mistral (–∫–∞—á–µ—Å—Ç–≤–æ/—Å–∫–æ—Ä–æ—Å—Ç—å)
- **–î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤**: Qwen (–±–∞–ª–∞–Ω—Å)

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SSD –¥–ª—è –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π
- –í—ã–¥–µ–ª–∏—Ç–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM (16GB+ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ `CUDA_VISIBLE_DEVICES` –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ GPU

## üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–æ–µ–∫—Ç–∞–º–∏

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤:
- `day_05/` - –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —á–∞—Ç-–±–æ—Ç–æ–º
- –ë—É–¥—É—â–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –º–æ–≥—É—Ç –ª–µ–≥–∫–æ –ø–æ–¥–∫–ª—é—á–∞—Ç—å—Å—è –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º

### –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

```python
# –í –ª—é–±–æ–º –ø—Ä–æ–µ–∫—Ç–µ
from local_models.client import LocalModelClient

client = LocalModelClient()
response = await client.chat("qwen", [
    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}
])
```

---

**üí° –°–æ–≤–µ—Ç**: –≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ –±–∞–∑–æ–≤–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞. –î–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏!
