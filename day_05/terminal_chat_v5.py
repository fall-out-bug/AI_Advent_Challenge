#!/usr/bin/env python3
"""
–¢–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã–π —á–∞—Ç v5: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π, —Ä–µ–∂–∏–º ¬´–æ–±—ä—è—Å–Ω—è–π¬ª,
–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã, —Ä–µ–∂–∏–º ¬´–¥–∞–π —Å–æ–≤–µ—Ç¬ª –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

–ö–æ–º–∞–Ω–¥—ã:
- temp <value>      ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É (–ø—Ä–∏–º–µ—Ä: temp 0.7)
- –æ–±—ä—è—Å–Ω—è–π          ‚Äî –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π (–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ –º–æ–¥–µ–ª—å)
- –Ω–∞–¥–æ–µ–ª            ‚Äî –≤—ã–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π
- –¥–∞–π —Å–æ–≤–µ—Ç         ‚Äî –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º —Å–æ–≤–µ—Ç—á–∏–∫–∞
- —Å—Ç–æ–ø —Å–æ–≤–µ—Ç        ‚Äî –≤—ã–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º —Å–æ–≤–µ—Ç—á–∏–∫–∞
- –º–æ–¥–µ–ª—å <name>     ‚Äî –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å (qwen, mistral, tinyllama)
- –ø–æ–∫–µ–¥–∞            ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä
"""
import asyncio
import sys
import os
import unicodedata
import shutil
import textwrap
import time
import httpx

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import get_api_key, is_api_key_configured
from advice_mode_v5 import AdviceModeV5
from temperature_utils import resolve_effective_temperature, clamp_temperature


# –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
LOCAL_MODELS = {
    "qwen": "http://localhost:8000",
    "mistral": "http://localhost:8001", 
    "tinyllama": "http://localhost:8002"
}

# –í–Ω–µ—à–Ω–∏–µ API (fallback)
API_URL = "https://api.perplexity.ai/chat/completions"
MODEL = "sonar-pro"
CHAD_URL = 'https://ask.chadgpt.ru/api/public/gpt-5-mini'


def _contains_any(haystack: str, needles: list[str]) -> bool:
    s = haystack.lower()
    return any(n in s for n in needles)


def _normalize_for_trigger(text: str) -> str:
    """
    Purpose: Normalize unicode and punctuation to improve trigger matching.
    """
    if not text:
        return ""
    # Unicode normalize and lower
    norm = unicodedata.normalize('NFKC', text).lower()
    # Replace common separators with space
    for ch in ["\u2013", "\u2014", "-", "_", "|", ",", ".", "!", "?", "\u00A0", "\u2019", "\u2018", "\u201c", "\u201d", "\"", "'", ":", ";", "(", ")", "[", "]", "{" , "}"]:
        norm = norm.replace(ch, " ")
    # Collapse spaces
    norm = " ".join(norm.split())
    return norm


class LocalModelClient:
    """
    Purpose: Client for local model APIs with conversation history support.
    """
    
    def __init__(self, model_url: str):
        self.url = model_url
        self.conversation_history = []
        
    async def chat(self, messages: list, max_tokens: int = 200, temperature: float = 0.7, use_history: bool = False) -> dict:
        """
        Purpose: Send chat request to local model with optional conversation history.
        Args:
            messages: List of message dicts with role and content
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            use_history: Whether to include conversation history
        Returns:
            dict: Response with text and token information
        """
        # Build messages with optional history
        if use_history:
            full_messages = self.conversation_history + messages
        else:
            full_messages = messages
        
        payload = {
            "messages": full_messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        try:
            async with httpx.AsyncClient(timeout=None) as client:
                response = await client.post(f"{self.url}/chat", json=payload)
                if response.status_code != 200:
                    return {
                        "response": f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {response.status_code}",
                        "input_tokens": 0,
                        "response_tokens": 0,
                        "total_tokens": 0
                    }
                
                data = response.json()
                if "response" not in data:
                    return {
                        "response": "‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏",
                        "input_tokens": 0,
                        "response_tokens": 0,
                        "total_tokens": 0
                    }
                
                result = {
                    "response": data["response"].strip(),
                    "input_tokens": data.get("input_tokens", 0),
                    "response_tokens": data.get("response_tokens", 0),
                    "total_tokens": data.get("total_tokens", 0)
                }
                
                # Update conversation history only if using history
                if use_history:
                    self.conversation_history.extend(messages)
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": result["response"]
                    })
                
                return result
        except Exception as e:
            return {
                "response": f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}",
                "input_tokens": 0,
                "response_tokens": 0,
                "total_tokens": 0
            }
    
    def clear_history(self):
        """
        Purpose: Clear conversation history.
        """
        self.conversation_history = []
    
    def get_history_length(self) -> int:
        """
        Purpose: Get current conversation history length.
        Returns:
            int: Number of messages in history
        """
        return len(self.conversation_history)


class DedChatV5:
    """
    Purpose: Terminal chat with temperature control, interactive switching, 
    experiment runner and local model support.
    """

    def __init__(self):
        self.api_key = None
        self.client = None
        self.default_temperature = 0.5
        self.explain_mode = False
        self.current_api = None  # "chadgpt", "perplexity", or local model name
        self._system_prompt = "–¢—ã –ø–æ–∂–∏–ª–æ–π —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –µ—Ö–∏–¥–Ω–æ –ø–æ–¥—à—É—á–∏–≤–∞–µ—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ª–∞–∫–æ–Ω–∏—á–Ω–æ, –±–µ–∑ —Å—Å—ã–ª–æ–∫."
        self.advice_mode: AdviceModeV5 | None = None
        self.local_clients = {name: LocalModelClient(url) for name, url in LOCAL_MODELS.items()}
        self.use_history = False  # Default: no history

    async def __aenter__(self):
        self.client = httpx.AsyncClient(timeout=30.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.aclose()

    def setup(self) -> bool:
        """
        Purpose: Ensure at least one API is configured.
        Returns:
            bool: True if configured
        """
        # Check if any local models are available
        if self._check_local_models():
            self.current_api = "mistral"  # Default to mistral
            return True
            
        # Prefer ChadGPT as primary API if configured
        if is_api_key_configured("chadgpt"):
            self.current_api = "chadgpt"
            self.api_key = get_api_key("chadgpt")
            return True
        # Fallback to Perplexity
        if is_api_key_configured("perplexity"):
            self.current_api = "perplexity"
            self.api_key = get_api_key("perplexity")
            return True
            
        print("‚ùå –ù–∏ –æ–¥–∏–Ω API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã!")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CHAD_API_KEY –∏–ª–∏ PERPLEXITY_API_KEY, –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        return False

    def _check_local_models(self) -> bool:
        """
        Purpose: Check if any local models are available.
        Returns:
            bool: True if at least one local model is available
        """
        # For now, assume local models are available if docker-compose is running
        # In production, you might want to ping each endpoint
        return True

    def print_welcome(self) -> None:
        """
        Purpose: Print welcome and help.
        """
        print("üë¥" + "=" * 60)
        print("üë¥  –î–ï–î–£–®–ö–ê AI v5 ‚Äî –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –ü–æ—è—Å–Ω—è–π, –°–æ–≤–µ—Ç—á–∏–∫ –∏ –õ–æ–∫–∞–ª—å–Ω—ã–µ –ú–æ–¥–µ–ª–∏")
        print("üë¥" + "=" * 60)
        print("üë£ –ö–∞–∫ –æ–±—â–∞—Ç—å—Å—è:")
        print("  ‚Ä¢ –ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã. –Ø –æ—Ç–≤–µ—á—É –µ—Ö–∏–¥–Ω–æ –∏ –ø–æ –¥–µ–ª—É.")
        print("  ‚Ä¢ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: –Ω–∏–∂–µ ‚Äî —Ç–æ—á–Ω–µ–µ, –≤—ã—à–µ ‚Äî —Å–º–µ–ª–µ–µ.")
        print()
        print("üîß –ö–æ–º–∞–Ω–¥—ã:")
        print("  ‚Ä¢ temp <value>         ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0.0‚Äì1.5)")
        print("  ‚Ä¢ –æ–±—ä—è—Å–Ω—è–π             ‚Äî –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π (–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å T –∏ –º–æ–¥–µ–ª—å)")
        print("  ‚Ä¢ –Ω–∞–¥–æ–µ–ª               ‚Äî –≤—ã–∫–ª—é—á–∏—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏—è –∏/–∏–ª–∏ —Å–æ–≤–µ—Ç—á–∏–∫–∞")
        print("  ‚Ä¢ –¥–∞–π —Å–æ–≤–µ—Ç            ‚Äî –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º —Å–æ–≤–µ—Ç—á–∏–∫–∞")
        print("  ‚Ä¢ –∏—Å—Ç–æ—Ä–∏—è –≤–∫–ª/–≤—ã–∫–ª     ‚Äî –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π")
        print("  ‚Ä¢ –æ—á–∏—Å—Ç–∏ –∏—Å—Ç–æ—Ä–∏—é       ‚Äî –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)")
        print("  ‚Ä¢ –ø–æ–∫–µ–¥–∞               ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä")
        print("  ‚Ä¢ api <provider>       ‚Äî –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å API (chadgpt, perplexity, local-qwen, local-mistral, local-tinyllama)")
        print()
        print("üì¶ –¢—Ä–∏–≥–≥–µ—Ä—ã –∞–≤—Ç–æ‚Äë—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (–ø–æ —Ç–µ–∫—Å—Ç—É –æ—Ç–≤–µ—Ç–∞):")
        print("  ‚Ä¢ '–Ω–µ –¥—É—à–Ω–∏' ‚Üí T=0.0   ‚Ä¢ '–ø–æ—Ç–∏—à–µ' ‚Üí T=0.7   ‚Ä¢ '—Ä–∞–∑–≥–æ–Ω—è–π' ‚Üí T=1.2")
        print()
        print("ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        for name, url in LOCAL_MODELS.items():
            status = "üü¢" if self.current_api == name else "‚ö™"
            print(f"  {status} {name} ‚Äî {url}")
        print()

    async def call_perplexity(self, message: str, temperature: float, system_prompt: str | None = None) -> str:
        """
        Purpose: Call Perplexity chat completion with given temperature.
        Args:
            message: User prompt
            temperature: Effective temperature
        Returns:
            str: Model reply
        """
        payload = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": (system_prompt or self._system_prompt)},
                {"role": "user", "content": message}
            ],
            "max_tokens": 800,
            "temperature": temperature
        }
        try:
            r = await self.client.post(
                API_URL,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json=payload
            )
            if r.status_code != 200:
                body = r.text
                preview = body[:240].replace("\n", " ") if body else ""
                return f"‚ùå –û—à–∏–±–∫–∞ API Perplexity: {r.status_code} {preview}"
            data = r.json()
            if "choices" not in data or not data["choices"]:
                return "‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç API Perplexity"
            return data["choices"][0]["message"]["content"].strip()
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ Perplexity: {e}"

    async def call_chadgpt(self, message: str, temperature: float, system_prompt: str | None = None) -> str:
        """
        Purpose: Call ChadGPT API. Temperature is not supported by API; retained for display.
        Args:
            message: User message
            temperature: Effective temperature (display only)
        Returns:
            str: Model reply
        """
        prompt_text = f"{(system_prompt or self._system_prompt)}\n\n–í–æ–ø—Ä–æ—Å: {message}"
        request_json = {
            "message": prompt_text,
            "api_key": self.api_key,
            "temperature": float(f"{temperature:.2f}"),
            "max_tokens": 800
        }
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                resp = await client.post(CHAD_URL, json=request_json)
                if resp.status_code != 200:
                    return f"‚ùå –û—à–∏–±–∫–∞ API: {resp.status_code}"
                data = resp.json()
                if data.get('is_success') and isinstance(data.get('response'), str):
                    return data['response'].strip()
                return "‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç API"
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞: {e}"

    async def call_local_model(self, message: str, temperature: float, system_prompt: str | None = None, use_history: bool = False) -> dict:
        """
        Purpose: Call local model API.
        Args:
            message: User message
            temperature: Effective temperature
            system_prompt: System prompt override
            use_history: Whether to use conversation history
        Returns:
            dict: Response with text and token information
        """
        if self.current_api not in self.local_clients:
            return {
                "response": f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {self.current_api}",
                "input_tokens": 0,
                "response_tokens": 0,
                "total_tokens": 0
            }
            
        messages = [
            {"role": "system", "content": (system_prompt or self._system_prompt)},
            {"role": "user", "content": message}
        ]
        
        return await self.local_clients[self.current_api].chat(
            messages=messages,
            max_tokens=800,
            temperature=temperature,
            use_history=use_history
        )

    async def call_model(self, message: str, temperature: float, system_prompt: str | None = None, use_history: bool = False) -> dict:
        """
        Purpose: Route call to current API (chadgpt/perplexity/local).
        Returns:
            dict: Response with text and optional token information
        """
        if self.current_api in self.local_clients:
            return await self.call_local_model(message, temperature, system_prompt, use_history)
        elif self.current_api == "chadgpt":
            response_text = await self.call_chadgpt(message, temperature, system_prompt)
            return {
                "response": response_text,
                "input_tokens": 0,
                "response_tokens": 0,
                "total_tokens": 0
            }
        else:
            response_text = await self.call_perplexity(message, temperature, system_prompt)
            return {
                "response": response_text,
                "input_tokens": 0,
                "response_tokens": 0,
                "total_tokens": 0
            }

    def parse_temp_override(self, user_message: str):
        """
        Purpose: Parse one-off override in 'temp=<v> <text>'.
        Args:
            user_message: Raw input
        Returns:
            (override: float|None, clean_text: str)
        """
        msg = user_message.strip()
        if not msg.lower().startswith("temp="):
            return None, msg
        try:
            head, rest = msg.split(maxsplit=1)
        except ValueError:
            head, rest = msg, ""
        try:
            override = float(head.split("=", 1)[1])
            override = clamp_temperature(override)
        except Exception as e:
            print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç temp=<value>: {e}")
            return None, rest.strip()
        return override, rest.strip()

    def apply_interactive_temperature(self, reply_text: str) -> None:
        """
        Purpose: Adjust default_temperature based on reply content.
        Rules:
            - contains "–Ω–µ –¥—É—à–Ω–∏" -> 0.0
            - contains "—Ä–∞–∑–≥–æ–Ω—è–π" -> 1.2
            - contains "–ø–æ—Ç–∏—à–µ"   -> 0.7
        """
        txt = _normalize_for_trigger(reply_text)
        if "–Ω–µ –¥—É—à–Ω–∏" in txt:
            self.default_temperature = clamp_temperature(0.0)
            print("üîß –ê–≤—Ç–æ–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: —Ç–µ–º–ø. = 0.0 (–ø–æ —Ñ—Ä–∞–∑–µ '–Ω–µ –¥—É—à–Ω–∏')\n")
        elif "—Ä–∞–∑–≥–æ–Ω—è–π" in txt:
            self.default_temperature = clamp_temperature(1.2)
            print("üîß –ê–≤—Ç–æ–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: —Ç–µ–º–ø. = 1.2 (–ø–æ —Ñ—Ä–∞–∑–µ '—Ä–∞–∑–≥–æ–Ω—è–π')\n")
        elif "–ø–æ—Ç–∏—à–µ" in txt:
            self.default_temperature = clamp_temperature(0.7)
            print("üîß –ê–≤—Ç–æ–ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: —Ç–µ–º–ø. = 0.7 (–ø–æ —Ñ—Ä–∞–∑–µ '–ø–æ—Ç–∏—à–µ')\n")

    def print_ded_message(self, message: str, eff_temp: float | None = None) -> None:
        """
        Purpose: Pretty-print model message with optional T-indicator.
        In explain_mode, temperature is always shown.
        Args:
            message: Text to print
            eff_temp: Effective temperature indicator
        """
        width = shutil.get_terminal_size((100, 20)).columns
        prefix = "üë¥ –î–µ–¥—É—à–∫–∞"
        model_label = self.get_model_label()
        if self.explain_mode:
            t = eff_temp if eff_temp is not None else self.default_temperature
            prefix = f"{prefix} [T={t:.2f} ‚Ä¢ {model_label}]"
        suffix = ""
        if not self.explain_mode:
            suffix = ""
        wrapped = textwrap.fill(f"{prefix}: {message}{suffix}", width=width)
        print(wrapped)
        print()

    def get_model_label(self) -> str:
        """
        Purpose: Human-readable model label for indicators.
        """
        if self.current_api in self.local_clients:
            return f"local:{self.current_api}"
        elif self.current_api == "chadgpt":
            return "chadgpt:gpt-5-mini"
        return f"perplexity:{MODEL}"

    def get_api_endpoint(self) -> str:
        """
        Purpose: Return API endpoint URL for current provider.
        """
        if self.current_api in self.local_clients:
            return LOCAL_MODELS[self.current_api]
        return CHAD_URL if self.current_api == "chadgpt" else API_URL

    def print_debug_info(self, user_message: str, reply_data: dict, eff_temp: float, sys_prompt: str | None, duration_ms: int) -> None:
        """
        Purpose: Print extended debug info in explain mode.
        """
        if not self.explain_mode:
            return
        width = shutil.get_terminal_size((100, 20)).columns
        model_label = self.get_model_label()
        endpoint = self.get_api_endpoint()
        mode = "advice" if sys_prompt else "normal"
        prompt_preview = (sys_prompt or self._system_prompt)[:120].replace("\n", " ")
        
        reply_text = reply_data.get("response", "")
        input_tokens = reply_data.get("input_tokens", 0)
        response_tokens = reply_data.get("response_tokens", 0)
        total_tokens = reply_data.get("total_tokens", 0)
        
        lines = [
            "üîç Debug:",
            f"  api: {self.current_api}",
            f"  endpoint: {endpoint}",
            f"  model: {model_label}",
            f"  temperature: {eff_temp:.2f}",
            f"  mode: {mode}",
            f"  prompt(sys)‚âà: {prompt_preview}",
            f"  req_len: {len(user_message)} chars",
            f"  resp_len: {len(reply_text)} chars",
            f"  time: {duration_ms} ms ({duration_ms/1000:.2f}s)",
        ]
        
        # Add token information for local models
        if self.current_api in self.local_clients and total_tokens > 0:
            lines.extend([
                f"  input_tokens: {input_tokens}",
                f"  response_tokens: {response_tokens}",
                f"  total_tokens: {total_tokens}"
            ])
        
        print(textwrap.fill("\n".join(lines), width=width))
        print()

    def get_user_input(self) -> str:
        """
        Purpose: Read user input with temperature indicator.
        Returns:
            str: Raw input
        """
        try:
            t = f"{self.default_temperature:.2f}"
            mode = " [–ü–û–Ø–°–ù–Ø–ô]" if self.explain_mode else ""
            model_label = self.get_model_label()
            return input(f"ü§î –í—ã{mode} [T={t} ‚Ä¢ {model_label}]: ").strip()
        except KeyboardInterrupt:
            print("\nüë¥ –î–µ–¥—É—à–∫–∞: –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            sys.exit(0)
        except EOFError:
            print("\nüë¥ –î–µ–¥—É—à–∫–∞: –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            sys.exit(0)

    async def run(self):
        """
        Purpose: Main loop.
        """
        if not self.setup():
            return
        self.print_welcome()
        # init advice mode helper (idle by default)
        self.advice_mode = AdviceModeV5(self.api_key, self.current_api)
        welcome_data = await self.call_model("–ü—Ä–∏–≤–µ—Ç! –î–∞–π –∫–æ—Ä–æ—Ç–∫–æ–µ –µ—Ö–∏–¥–Ω–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ.", self.default_temperature, use_history=self.use_history)
        self.print_ded_message(welcome_data["response"], self.default_temperature)
        self.apply_interactive_temperature(welcome_data["response"])

        while True:
            user_message = self.get_user_input()
            if not user_message:
                continue

            low = user_message.lower()

            # exit by substring (only "–ø–æ–∫–µ–¥–∞")
            if _contains_any(low, ["–ø–æ–∫–µ–¥–∞"]):
                bye_data = await self.call_model("–°–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ –µ—Ö–∏–¥–Ω–æ–µ –ø—Ä–æ—â–∞–Ω–∏–µ.", self.default_temperature, use_history=self.use_history)
                self.print_ded_message(bye_data["response"], self.default_temperature)
                self.apply_interactive_temperature(bye_data["response"])
                break

            # API switching: 'api chadgpt', 'api perplexity', 'api local-qwen', etc.
            if low.startswith("api "):
                new_api = low.split()[1]
                
                # Handle local models with 'local-' prefix
                if new_api.startswith("local-"):
                    local_model = new_api[6:]  # Remove 'local-' prefix
                    if local_model in LOCAL_MODELS:
                        self.current_api = local_model
                        # Re-init advice mode helper with new API
                        self.advice_mode = AdviceModeV5(self.api_key, self.current_api)
                        print(f"üë¥ –î–µ–¥—É—à–∫–∞: –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å {local_model}!")
                        print()
                    else:
                        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å '{local_model}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {', '.join(LOCAL_MODELS.keys())}")
                        print()
                # Handle external APIs
                elif new_api in ("chadgpt", "perplexity"):
                    if not is_api_key_configured(new_api):
                        print(f"‚ùå API '{new_api}' –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á.")
                        print()
                    else:
                        self.current_api = new_api
                        self.api_key = get_api_key(new_api)
                        # Re-init advice mode helper with new API
                        self.advice_mode = AdviceModeV5(self.api_key, self.current_api)
                        print(f"üë¥ –î–µ–¥—É—à–∫–∞: –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ {new_api}!")
                        print()
                else:
                    available_apis = ["chadgpt", "perplexity"] + [f"local-{name}" for name in LOCAL_MODELS.keys()]
                    print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π API '{new_api}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {', '.join(available_apis)}")
                    print()
                continue

            # explain mode on/off by substring
            if "–æ–±—ä—è—Å–Ω—è–π" in low:
                self.explain_mode = True
                print("üë¥ –î–µ–¥—É—à–∫–∞: –í–∫–ª—é—á–∏–ª —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π (—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å—Å—è).")
                print()
                continue

            if "–Ω–∞–¥–æ–µ–ª" in low:
                # Only turns off explain mode (not advice)
                self.explain_mode = False
                print("üë¥ –î–µ–¥—É—à–∫–∞: –í—ã–∫–ª—é—á–∏–ª —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π.")
                print()
                # do not continue; allow message to be processed below

            # history toggle command
            if "–∏—Å—Ç–æ—Ä–∏—è –≤–∫–ª" in low:
                self.use_history = True
                print("üë¥ –î–µ–¥—É—à–∫–∞: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤–∫–ª—é—á–µ–Ω–∞!")
                print()
                continue
            
            if "–∏—Å—Ç–æ—Ä–∏—è –≤—ã–∫–ª" in low:
                self.use_history = False
                print("üë¥ –î–µ–¥—É—à–∫–∞: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤—ã–∫–ª—é—á–µ–Ω–∞!")
                print()
                continue

            # clear history command
            if "–æ—á–∏—Å—Ç–∏ –∏—Å—Ç–æ—Ä–∏—é" in low:
                if self.current_api in self.local_clients:
                    self.local_clients[self.current_api].clear_history()
                    print("üë¥ –î–µ–¥—É—à–∫–∞: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –æ—á–∏—â–µ–Ω–∞!")
                else:
                    print("üë¥ –î–µ–¥—É—à–∫–∞: –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")
                print()
                continue

            # help by substring
            if "–ø–æ–º–æ–≥–∞–π" in low:
                print("üîß –ö–æ–º–∞–Ω–¥—ã:")
                print("  ‚Ä¢ temp <value>         ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0.0‚Äì1.5)")
                print("  ‚Ä¢ –æ–±—ä—è—Å–Ω—è–π             ‚Äî –≤–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π (–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å T –∏ –º–æ–¥–µ–ª—å)")
                print("  ‚Ä¢ –Ω–∞–¥–æ–µ–ª               ‚Äî –≤—ã–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ—è—Å–Ω–µ–Ω–∏–π")
                print("  ‚Ä¢ –∏—Å—Ç–æ—Ä–∏—è –≤–∫–ª/–≤—ã–∫–ª     ‚Äî –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π")
                print("  ‚Ä¢ –æ—á–∏—Å—Ç–∏ –∏—Å—Ç–æ—Ä–∏—é       ‚Äî –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π)")
                print("  ‚Ä¢ –ø–æ–∫–µ–¥–∞               ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä")
                print("  ‚Ä¢ api <provider>       ‚Äî –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å API (chadgpt, perplexity, local-qwen, local-mistral, local-tinyllama)")
                print()
                print("üì¶ –¢—Ä–∏–≥–≥–µ—Ä—ã –∞–≤—Ç–æ‚Äë—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (–ø–æ —Ç–µ–∫—Å—Ç—É): '–Ω–µ –¥—É—à–Ω–∏' ‚Üí 0.0, '–ø–æ—Ç–∏—à–µ' ‚Üí 0.7, '—Ä–∞–∑–≥–æ–Ω—è–π' ‚Üí 1.2")
                print()
                continue

            if low.startswith("temp "):
                try:
                    value = float(user_message.split()[1])
                    self.default_temperature = clamp_temperature(value)
                    print(f"üë¥ –î–µ–¥—É—à–∫–∞: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é {self.default_temperature}")
                except Exception as e:
                    print(f"‚ùå {e}")
                print()
                continue

            # Apply temperature triggers from USER message as well
            self.apply_interactive_temperature(user_message)
            # If user message contained any control words, do not forward it to API
            if any(k in low for k in ["–æ–±—ä—è—Å–Ω—è–π", "–Ω–∞–¥–æ–µ–ª", "–ø–æ–º–æ–≥–∞–π"]) or low.startswith("temp "):
                continue

            # Advice mode triggers and routing (from day_03 behavior)
            if "–¥–∞–π —Å–æ–≤–µ—Ç" in low:
                print("üë¥ –î–µ–¥—É—à–∫–∞: –í–∫–ª—é—á–∞—é —Ä–µ–∂–∏–º —Å–æ–≤–µ—Ç—á–∏–∫–∞...")
                print()
                # Prime advice mode with the user message
                print("üë¥ –î–µ–¥—É—à–∫–∞ –ø–µ—á–∞—Ç–∞–µ—Ç...", end="", flush=True)
                reply = await self.advice_mode.handle_advice_request(user_message)
                print("\r" + " " * 30 + "\r", end="")
                self.print_ded_message(rely := reply, self.default_temperature)
                self.apply_interactive_temperature(rely)
                continue

            if self.advice_mode and self.advice_mode.is_advice_mode_active():
                print("üë¥ –î–µ–¥—É—à–∫–∞ –ø–µ—á–∞—Ç–∞–µ—Ç...", end="", flush=True)
                reply = await self.advice_mode.handle_advice_request(user_message)
                print("\r" + " " * 30 + "\r", end="")
                self.print_ded_message(rely2 := reply, self.default_temperature)
                self.apply_interactive_temperature(rely2)
                # If advice session ended internally after final advice, continue to next turn
                if not self.advice_mode.is_advice_mode_active():
                    print()
                continue

            # per-message temp override removed; always use default_temperature
            eff = self.default_temperature
            print("üë¥ –î–µ–¥—É—à–∫–∞ –ø–µ—á–∞—Ç–∞–µ—Ç...", end="", flush=True)
            start_ts = time.time()
            reply_data = await self.call_model(user_message, eff, use_history=self.use_history)
            duration_ms = int((time.time() - start_ts) * 1000)
            print("\r" + " " * 30 + "\r", end="")
            self.print_ded_message(reply_data["response"], eff)
            # Debug info (explain mode only)
            self.print_debug_info(user_message, reply_data, eff, None, duration_ms)
            self.apply_interactive_temperature(reply_data["response"])


async def main():
    chat = DedChatV5()
    async with chat:
        await chat.run()


if __name__ == "__main__":
    asyncio.run(main())
