# Архитектурный отчет: обработка сообщений, настройки моделей и работа с промптами

Дата: 2025-10-31

## 1) Сквозной поток сообщения

- Пользователь → Telegram
  - Точка входа: `src/presentation/bot/butler_bot.py` → `ButlerBot.handle_natural_language`
  - Контекст: `user_id`, `session_id`, `dialog_context` (через `DialogManager`/MongoDB)
- Агентная прослойка
  - `src/domain/agents/mcp_aware_agent.py` → `MCPAwareAgent.process(request)`
  - Открывает список тулов через MCP (`/tools`), строит системный промпт, вызывает модель, парсит ответ; при наличии инструмента — выполняет тул
  - Возвращает: основной текст + reasoning-отчет (как второе сообщение)
- MCP сервер (HTTP 8004)
  - `src/presentation/mcp/server.py` регистрирует тулы (`digest_tools`, `pdf_digest_tools`, и др.)
  - Выполняет вызовы тулов и отдает JSON-результаты
- LLM-клиент
  - `shared/shared_package/clients/unified_client.py` решает через конфиг: OpenAI-совместимый `/v1/chat/completions` или локальный `/chat`
  - Ретрай, таймауты, метрики
- Локальная модель
  - `archive/legacy/local_models/chat_api.py` (FastAPI)
  - Эндпоинты: `/v1/chat/completions` (OpenAI), `/v1/models`, `/chat` (legacy), `/metrics`

## 2) Ключевые точки настройки модели

- Выбор модели на уровне агента
  - В `MCPAwareAgent`: `model_name="mistral"`, `max_tokens=2048`, `temperature=0.7`
- На уровне локальной модели (`chat_api.py`)
  - Ограничения ввода/вывода: `MAX_INPUT_TOKENS` (env, default 4096), `MAX_OUTPUT_TOKENS` (env, default 1024)
  - Генерация: `max_tokens` (из запроса, clamped ≤ `MAX_OUTPUT_TOKENS`), `temperature`, `top_k`, `top_p`
  - Форматирование промпта под семейства моделей:
    - Mistral/OpenHermes/Mixtral: `[INST] <<SYS>> … <</SYS>> … [/INST]`
    - Starcoder instruct: `### Instruction … ### Response`
    - Qwen-Chat: собственный формат
- OpenAI-совместимость
  - `/v1/chat/completions`: принимает `messages[]`, возвращает `choices`, `usage`
  - `/v1/models` и `/v1/models/{id}` — для health-check и наличия модели
- Мониторинг
  - `/metrics` (Prometheus), Графана дашборды
  - Счетчики: запросы, длительность, токены, загрузка модели

## 3) Работа с промптами (prompt plumbing)

- Что формируем
  - Системный промпт-рамка: правила работы с каналами, явные инструкции, формат вызова тулов
  - Вставка «списка инструментов» (сейчас компактный, релевантный для каналов/дайджеста)
  - Пользовательский текст + `User ID` (+ контекст сессии при наличии)
- Как передаем в модель
  - Агент собирает финальный строковый промпт → `UnifiedModelClient.make_request` → локальная модель (`/v1/chat/completions` либо `/chat`)
  - В `chat_api.build_prompt` промпт приводится к формату конкретной модели
- Гигиена промпта и защита от «эхо»
  - Сократили список тулов → фильтруем нужные (digest/channels/pdf)
  - Правило: «НЕ повторяй промпт/инструкции в ответе»
  - Парсер ответа удаляет артефакты (`[INST]`, `<<SYS>>`, `### Instruction/Response`, «Available tools»), пытается извлечь JSON вызываемого тула
- Управление длиной ответа
  - В боте: если ответ > 4000 символов → автогенерация PDF через MCP `convert_markdown_to_pdf`
  - Иначе — текст
- Доп. выход
  - Вторым сообщением бот отправляет reasoning-отчет (что делал агент, какие тулы, параметры, длительность)

## 4) MCP-инструменты и их использование

- Каналы/дайджест:
  - `get_channel_digest_by_name` — автоподписка и дайджест по каналу
  - `get_channel_digest` — дайджест по всем подпискам
  - `list_channels`, `add_channel`, `get_channel_metadata`
- PDF-пайплайн:
  - `get_posts_from_db` → `summarize_posts` → `format_digest_markdown` → `combine_markdown_sections` → `convert_markdown_to_pdf`
- Lazy-импорты: Pyrogram/transformers импортируются внутри функций → стабильная регистрация тулов

## 5) Текущие симптомы и причины «стены текста»

- Симптом: модель печатает системный промпт и «всё подряд», не вызывает тул
- Причины:
  - Склонность Mistral «эхо-ить» входной промпт
  - Слишком подробные списки/инструкции провоцируют «объяснительный» ответ
  - «JSON-вызов» теряется в тексте; парсер не всегда извлекает

## 6) Рекомендации по тюнингу (быстрые)

- Сжать системный промпт ещё сильнее:
  - Оставить 3–5 ключевых тулов: `get_channel_digest_by_name`, `get_channel_digest`, `list_channels`, `add_channel`, `get_channel_metadata`
  - Убрать второстепенные тулы (PDF-пайплайн) из шага «принятия решения»
- Добавить few-shot примеры ровно в требуемом формате (только JSON без текста)
- Снизить `temperature` до 0.2–0.3 на шаге выбора инструмента
- Уменьшить `max_tokens` на шаге выбора инструмента до 256–512
- Включить «force tool mode» при детекции «дайджест» (regex):
  - При матчах агент формирует JSON-вызов напрямую, LLM — только для форматирования результата
- Ограничить правила:
  - Явно: «Верни ТОЛЬКО JSON для инструмента. Без текста». (шаг решения)
  - Шаг форматирования — отдельный системный промпт без списка тулов

## 7) Настройки и параметры (для тестов)

- Агент
  - model_name: `mistral`
  - temperature: `0.2–0.3`
  - max_tokens: `256–512` (шаг выбора тула)
- Локальная модель (`chat_api.py`)
  - `MAX_INPUT_TOKENS`: 4096 (env)
  - `MAX_OUTPUT_TOKENS`: 512–768 (шаг выбора тула)
- Клиент MCP
  - Таймауты: discovery 5s, execution 30s
- Бот
  - >4000 символов → PDF; reasoning — вторым сообщением

## 8) План поэтапного тюнинга

- Этап A — строгий инструментальный режим (Decision-only)
  - Системный промпт «реши инструмент» с «Ответь только JSON» (+ few-shot)
  - max_tokens=256, temperature=0.2
  - Парсим JSON → вызываем тул → отдельным шагом форматируем итог
- Этап B — тонкая настройка контента
  - Для форматирования результата другой системный промпт без списка тулов
  - Разделение «решение» и «форматирование» уменьшает «эхо»
- Этап C — контроль длины
  - Если формат >4000 → PDF (как сейчас)
  - «Краткая версия» + «полная версия в PDF»

## 9) Диагностика и метрики

- Логи:
  - MCP Server: кол-во тулов, регистрация модулей, импорты
  - Бот: HTTP к `/v1/chat/completions`, текст ответа, ошибки Telegram
- Метрики:
  - Агент: успех/ошибка, длительность, токены
  - Локальная модель: запросы, длительность, токены, загрузка
- Трассировка:
  - correlation id (session_id) на всем пути

---

Контакт для A/B промптов: подготовить «Decision-only» промпт с 2–3 few-shot примерами и включить переключатель режима в агенте для сравнения.
