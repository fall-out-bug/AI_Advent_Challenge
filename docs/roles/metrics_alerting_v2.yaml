# Metrics & Alerting Configuration v2.0
# Comprehensive observability for agent system

version: 2.0
retention_days: 90
aggregation_interval: 1_minute

# System-Wide Metrics
system_metrics:
  efficiency:
    token_usage:
      metric: counter
      labels: [agent_role, epic_id, action]
      alert: daily_usage > budget
      dashboard: token_efficiency

    consensus_time:
      metric: histogram
      buckets: [1, 5, 15, 30, 60, 120]  # minutes
      alert: p95 > 30_minutes
      dashboard: consensus_performance

    iteration_count:
      metric: histogram
      buckets: [1, 2, 3]
      alert: avg > 2.5
      dashboard: consensus_efficiency

  quality:
    architecture_drift:
      metric: gauge
      formula: violations / total_components
      alert: value > 0.15
      dashboard: architecture_health

    test_coverage:
      metric: gauge
      labels: [component, type]
      alert: value < 0.80
      dashboard: quality_metrics

    production_incidents:
      metric: counter
      labels: [severity, root_cause]
      alert: rate_1h > 0
      dashboard: production_health

  delivery:
    epic_velocity:
      metric: gauge
      formula: epics_completed / time_period
      alert: value < 0.8 * baseline
      dashboard: delivery_performance

    lead_time:
      metric: histogram
      buckets: [4, 8, 16, 24, 48, 72]  # hours
      alert: p95 > 48_hours
      dashboard: delivery_metrics

# Per-Agent Metrics
agent_metrics:
  analyst:
    requirements_clarity:
      metric: gauge
      formula: accepted_first_time / total_requirements
      alert: value < 0.7
      dashboard: analyst_performance

    scope_creep:
      metric: gauge
      formula: (final_scope - initial_scope) / initial_scope
      alert: value > 0.2
      dashboard: scope_management

  architect:
    layer_violations:
      metric: counter
      labels: [layer_from, layer_to]
      alert: rate_1h > 0
      dashboard: architecture_violations

    technical_debt:
      metric: gauge
      unit: hours
      alert: value > 40
      dashboard: tech_debt_tracker

  tech_lead:
    task_accuracy:
      metric: gauge
      formula: tasks_completed_as_planned / total_tasks
      alert: value < 0.8
      dashboard: planning_accuracy

    staging_success:
      metric: gauge
      formula: successful_deployments / total_deployments
      alert: value < 0.9
      dashboard: deployment_success

  developer:
    tdd_compliance:
      metric: gauge
      formula: tests_written_first / total_commits
      alert: value < 0.95
      dashboard: tdd_metrics

    defect_rate:
      metric: gauge
      formula: bugs_found / lines_of_code * 1000
      alert: value > 5
      dashboard: code_quality

  quality:
    escape_rate:
      metric: gauge
      formula: production_bugs / total_bugs
      alert: value > 0.05
      dashboard: quality_effectiveness

    review_efficiency:
      metric: gauge
      formula: issues_found / review_time_hours
      alert: value < 2
      dashboard: review_metrics

  devops:
    deployment_frequency:
      metric: gauge
      unit: per_day
      alert: value < 1
      dashboard: deployment_metrics

    mttr:
      metric: gauge
      unit: minutes
      alert: value > 30
      dashboard: incident_response

    availability:
      metric: gauge
      formula: uptime / total_time
      alert: value < 0.999
      dashboard: sla_tracking

# Alerting Rules
alerts:
  critical:  # Page immediately
    - architecture_violation_detected
    - production_down
    - security_vulnerability
    - data_loss_risk
    - sla_breach

  high:  # Notify within 15 minutes
    - test_coverage_drop
    - consensus_timeout
    - deployment_failure
    - performance_degradation
    - error_rate_spike

  medium:  # Notify within 1 hour
    - token_budget_80_percent
    - tech_debt_increasing
    - scope_creep_detected
    - review_backlog_growing

  low:  # Daily summary
    - documentation_outdated
    - pattern_optimization_available
    - agent_performance_below_baseline

# Dashboard Configuration
dashboards:
  executive_summary:
    refresh: 5_minutes
    panels:
      - epic_progress: gauge
      - system_health: traffic_light
      - token_spend: burn_rate
      - delivery_velocity: trend_line
      - quality_score: radar_chart

  agent_performance:
    refresh: 1_minute
    panels:
      - consensus_time: heatmap
      - veto_frequency: bar_chart
      - decision_accuracy: line_graph
      - token_efficiency: stacked_area

  quality_metrics:
    refresh: 1_minute
    panels:
      - test_coverage: gauge_grid
      - code_complexity: treemap
      - issue_distribution: pie_chart
      - review_effectiveness: scatter_plot

  production_health:
    refresh: 30_seconds
    panels:
      - error_rate: time_series
      - response_time: percentiles
      - throughput: counter
      - availability: uptime_bar

  cost_management:
    refresh: 1_hour
    panels:
      - token_usage_by_agent: stacked_bar
      - cost_per_epic: trend_line
      - roi_analysis: comparison
      - budget_forecast: projection

# Metric Export Configuration
export:
  prometheus:
    endpoint: /metrics
    format: prometheus_text
    scrape_interval: 15s

  grafana:
    datasource: prometheus
    dashboards_path: /dashboards/
    auto_provision: true

  datadog:
    api_key: ${DD_API_KEY}
    tags: [environment, agent_role]
    flush_interval: 10s

# Performance Targets (SLOs)
slos:
  consensus:
    target: 95% within 30 minutes
    measurement_window: 7_days

  quality:
    target: 0 critical issues in production
    measurement_window: 30_days

  availability:
    target: 99.9% uptime
    measurement_window: 30_days

  delivery:
    target: 80% epics on time
    measurement_window: 30_days

  efficiency:
    target: token usage within budget
    measurement_window: per_epic

# Automated Actions
auto_remediation:
  on_token_budget_exceeded:
    - switch_to_lower_tier_models
    - increase_compression
    - reduce_iteration_limit

  on_consensus_timeout:
    - escalate_to_human
    - log_pattern_for_analysis
    - suggest_rule_adjustment

  on_quality_degradation:
    - increase_review_depth
    - add_mandatory_gates
    - require_additional_tests

  on_performance_issue:
    - scale_infrastructure
    - enable_caching
    - optimize_queries

# Reporting
reports:
  daily:
    recipients: [tech_lead, product_owner]
    content:
      - epic_progress
      - token_usage
      - quality_metrics
      - blockers

  weekly:
    recipients: [stakeholders]
    content:
      - delivery_velocity
      - cost_analysis
      - quality_trends
      - improvement_suggestions

  monthly:
    recipients: [executives]
    content:
      - roi_analysis
      - slo_compliance
      - cost_optimization
      - strategic_recommendations
