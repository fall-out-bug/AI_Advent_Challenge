# Stage 03_01 SLO & SLI Recommendations

| Component | User Journey | SLI Definition | Target / SLO | Measurement Notes | Alert Policy |
|-----------|--------------|----------------|--------------|-------------------|--------------|
| Review Pipeline (MCP + Unified Worker + Modular Reviewer) | Student submits review request → report delivered | `review_pipeline_latency_seconds` (histogram) measuring time from MCP `/call` request accepted to final report stored. | 95% ≤ 300s, 99% ≤ 600s; error rate <1% per rolling day. | Emit start/end events in MCP server and unified worker; export histogram + counters; correlate with queue depth. | Burn-rate alert: P95 > 300s for 15m (warning), P99 > 600s for 15m (critical); error_rate >1% for 30m (critical). |
| MCP HTTP Server | Consumers invoking MCP tools | Availability via `up{job="mcp-server"}` and success via `mcp_requests_total{status="success"}` vs failure. Latency via `mcp_request_duration_seconds`. | Availability ≥ 99.0% monthly; P95 latency ≤ 2s. | Instrument FastAPI middleware for status codes and duration; ensure `/metrics` exports histogram/counter. | Alert when availability drops below 99% over 1h (critical) and when P95 latency > 2s for 10m (warning). |
| Butler Bot Presentation Layer | Telegram users interacting with bot | Success ratio: `butler_messages_total{status="success"} / sum(butler_messages_total)`; latency: `butler_message_duration_seconds` P95. | Success ≥ 98%, P95 latency ≤ 5s. | Ensure metrics labels reflect intents/handlers; include user_id (hashed) for sampling. | Alert on success ratio < 98% for 15m (critical) and P95 latency > 5s for 10m (warning). |
| Unified Task Worker (Queue Processing) | Background jobs (reviews, digests) | Throughput: `unified_worker_tasks_total{status="completed"}` vs `failed`; queue depth gauge; job duration histogram. | 99% jobs complete successfully; median queue wait < 30s. | Emit Prometheus counters in `src/workers`; poll Mongo queue depth into gauge. | Alert when failure rate ≥ 5% for 10m (critical) or queue depth > 50 for 15m (warning). |
| Post Fetcher Worker | Channel harvesting | Success ratio of `post_fetcher_posts_saved_total` vs errors; freshness via `post_fetcher_last_run_timestamp_seconds`. | 99% successful runs; max staleness 2h. | Already instrumented counters/gauges but ensure scheduler updates timestamp. | Alert when `time() - post_fetcher_last_run_timestamp_seconds > 2h` (critical) or error rate > 10% for 30m (warning). |
| LLM Backend (Shared) | MCP/Worker LLM calls | Histogram `llm_inference_latency_seconds` P95; counter `llm_error_rate_total`. | P95 latency ≤ 5s; error rate < 5%. | Add middleware in `shared/clients` to increment counters; capture model label. | Alert when P95 latency > 5s for 10m (warning) or error rate ≥ 10% for 5m (critical). |
| Observability Platforms | Prometheus scraping + Grafana dashboards | Prometheus scrape success (`up`), Grafana datasource health checks. | 99.9% Prometheus scrape success; dashboards load < 2s (sample). | Use `prometheus_target_interval_length_seconds` for scrape health; add Grafana synthetic check script. | Alert when target scrape failures > 5% for 10m; Grafana health endpoint failure (critical). |

## Implementation Guidelines
- **Metric Naming**: adopt consistent prefixes (`mcp_`, `review_pipeline_`, `worker_`) and align with [Prometheus naming best practices](https://prometheus.io/docs/practices/naming/).
- **SLI Storage**: Persist daily SLI calculations (e.g., to Mongo collection `observability_sli_metrics`) to support historical error-budget tracking.
- **Alert Routing**: When Alertmanager is enabled, map warnings to on-call chat (Telegram/Slack) and critical alerts to paging/phone escalation.
- **Error Budgets**: For each SLO, compute monthly error budget = (1 - target) × total minutes; document burn thresholds (2% budget in 1h = page).
- **Dashboard Updates**: Add Grafana panels showing SLO attainment (red/green) and cumulative error budget consumption for each component.

