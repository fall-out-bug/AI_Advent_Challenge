{
  "epic_id": "epic_27",
  "iteration": 1,
  "timestamp": "2025_11_20_14_05_00",
  "user_story": "As a developer, I want the local Test Agent to generate high-quality unit and integration tests for large Python modules so that I can trust local LLM-based testing as much as remote agents.",
  "acceptance_criteria": [
    "GIVEN a Python module or package directory with multiple files WHEN the developer runs the local Test Agent on that directory THEN the agent generates pytest-based unit and integration tests that execute successfully without manual fixes.",
    "GIVEN an existing set of tests written by Cursor agents for a comparable code area WHEN the same area is tested by the local Test Agent using Qwen THEN overall test coverage for the local agent is on the same level (≥80%) even if some narrow edge cases are missed.",
    "GIVEN a large codebase segment (multiple modules) WHEN the local Test Agent processes it with a limited context window (approx. 4000 tokens) THEN it uses sliding and summarisation strategies to cover all relevant public behaviours with tests, without crashing or timing out in normal project conditions."
  ],
  "user_testing_strategy": {
    "e2e_required": true,
    "user_action_simulation": true,
    "simulation_scripts_location": "scripts/e2e/epic_27/",
    "scenarios": [
      "Scenario 1: User points the Test Agent CLI to a medium-sized module directory; the agent generates and saves tests, then the user runs pytest and verifies that tests pass and coverage is at or above 80%.",
      "Scenario 2: User selects a code area that already has tests written by Cursor agents; they run the local Test Agent on the same area, run pytest for both sets, and compare coverage and failure rate to confirm the local agent is comparable in usefulness.",
      "Scenario 3: User points the Test Agent to a larger package (multiple modules); the agent processes files in chunks, generates tests across the package, and the user verifies that there are meaningful tests for the main public APIs without the agent failing due to context limits."
    ]
  },
  "out_of_scope": [
    "Non-Python language support in the Test Agent.",
    "New graphical or web UI for the Test Agent beyond existing CLI and documentation.",
    "Performance and load testing of the LLM itself (focus is functional test quality and coverage).",
    "Large-scale refactoring of production code purely to make it more testable for the agent."
  ],
  "metrics": {
    "technical_metrics": [
      "Target coverage: For any selected module/package under test, pytest coverage with locally generated tests must reach at least 80% line coverage, in line with project standards.",
      "Comparative coverage: On code areas where Cursor agents already provide tests, coverage with local Qwen-generated tests must not be significantly worse (same order of magnitude, allowing some missed narrow edge cases).",
      "Stability on large modules: The Test Agent must successfully complete generation and test execution for modules/packages of the size commonly used in this repository (multiple files, typical domain/application modules) without errors due to context window handling."
    ],
    "customer_estimates": [
      "Customer expects most tests generated by the local agent to work immediately without manual fixes (tests should run and pass on first execution).",
      "Customer expects local Qwen-based tests to reach a similar coverage level to tests written by Cursor agents, accepting that some very narrow edge cases may be missed.",
      "Customer considers coverage the primary quantitative metric; other aspects such as chunking strategies and scaling to modules are important but should be handled transparently by the agent."
    ]
  },
  "constraints": [
    "The Test Agent must continue to follow the existing Clean Architecture structure (domain, application, infrastructure, presentation) without violating layer boundaries.",
    "Local LLM usage must rely on the existing Qwen integration; non-local LLMs (Cursor agents) are used only as an external comparison baseline, not as a dependency for the local Test Agent runtime.",
    "All generated tests must follow project standards: pytest, PEP 8, full type hints and docstrings where applicable, and target ≥80% coverage in line with repository rules.",
    "The agent must operate effectively within an approximate 4000-token context window, using sliding windows and summarisation or similar strategies to handle large modules and packages.",
    "Existing features from Epic 26 (Test Agent) must not be broken; Epic 27 refines quality and scalability rather than changing core behaviour."
  ]
}
