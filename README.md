# AI Advent Challenge

[English](README.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README.ru.md)

> Daily AI-powered projects exploring language models and multi-agent systems

## ğŸ¤– Quick Start for AI Agents

**Repository Structure:**
- `local_models/` - Local language model infrastructure (shared module)
- `shared/` - SDK for unified model interaction (shared module)
- `day_01/` to `day_04/` - Learning projects exploring basics
- `day_05/`, `day_06/` - Production projects using SDK
- `day_07/` - Multi-Agent System for code generation and review
- Each `day_XX/` is a standalone project with its own dependencies and tests

**Key Components:**
- **Local Models**: FastAPI servers in `local_models/` (ports 8000-8002)
- **SDK**: Unified interface for model interaction in `shared/`
- **Chat Bots**: Terminal applications with AI-powered interactions
- **Advisor Mode**: Structured dialogue with model constraints
- **Unified API**: `api <provider>` command for all models
- **Multi-Agent System**: Specialized agents for code generation and review

**Quick Start for Agents:**
1. Study `local_models/README.md` - local model architecture
2. Choose project `day_XX/` by complexity
3. Run `make install && make run` in selected project

## ğŸ“ Project Structure

```
AI_Advent_Challenge/
â”œâ”€â”€ .gitignore              # Ignored files
â”œâ”€â”€ config.py              # API key configuration (shared)
â”œâ”€â”€ api_key.txt.example    # Template for API keys
â”œâ”€â”€ Makefile              # Project setup commands
â”œâ”€â”€ README.md             # This file (English)
â”œâ”€â”€ README.ru.md          # Russian version
â”œâ”€â”€ AGENTS.md             # AI agents documentation (English)
â”œâ”€â”€ AGENTS.ru.md          # AI agents documentation (Russian)
â”œâ”€â”€ AGENTS_QUICK_REFERENCE.md # Quick reference (English)
â”œâ”€â”€ AGENTS_QUICK_REFERENCE.ru.md # Quick reference (Russian)
â”œâ”€â”€ local_models/         # ğŸ  Local language model infrastructure
â”‚   â”œâ”€â”€ chat_api.py       # FastAPI server for local models
â”‚   â”œâ”€â”€ docker-compose.yml # Docker Compose configuration
â”‚   â”œâ”€â”€ Dockerfile        # Image for running models
â”‚   â”œâ”€â”€ download_model.py # Script for pre-downloading models
â”‚   â””â”€â”€ README.md         # Local models documentation
â”œâ”€â”€ shared/               # ğŸ› ï¸ SDK for unified model interaction
â”‚   â”œâ”€â”€ config/          # Model configuration and constants
â”‚   â”œâ”€â”€ clients/         # Model clients
â”‚   â”œâ”€â”€ exceptions/      # Standardized exceptions
â”‚   â”œâ”€â”€ tests/           # SDK tests (98.59% coverage)
â”‚   â”œâ”€â”€ pyproject.toml   # SDK dependencies
â”‚   â””â”€â”€ README.md        # SDK documentation
â”œâ”€â”€ day_01/               # Day 1 - Terminal chat with AI
â”‚   â”œâ”€â”€ terminal_chat.py  # Main application
â”‚   â”œâ”€â”€ pyproject.toml    # Poetry dependencies
â”‚   â”œâ”€â”€ .venv/           # Poetry virtual environment
â”‚   â”œâ”€â”€ Makefile         # Development commands
â”‚   â””â”€â”€ README.md        # Project documentation
â”œâ”€â”€ day_02/               # Day 2 - Improved chat with JSON responses
â”‚   â”œâ”€â”€ terminal_chat_v2.py # Improved application
â”‚   â”œâ”€â”€ pyproject.toml    # Poetry dependencies
â”‚   â”œâ”€â”€ .venv/           # Poetry virtual environment
â”‚   â”œâ”€â”€ Makefile         # Development commands
â”‚   â””â”€â”€ README.md        # Project documentation
â”œâ”€â”€ day_03/               # Day 3 - Advisor mode with model constraints
â”‚   â”œâ”€â”€ terminal_chat_v3.py # Chat with advisor mode
â”‚   â”œâ”€â”€ advice_session.py   # Session state management
â”‚   â”œâ”€â”€ advice_mode.py      # Advisor mode logic
â”‚   â”œâ”€â”€ tests/             # Tests for all components
â”‚   â”œâ”€â”€ pyproject.toml     # Poetry dependencies
â”‚   â”œâ”€â”€ Makefile          # Development commands
â”‚   â””â”€â”€ README.md         # Project documentation
â”œâ”€â”€ day_04/               # Day 4 - Improved advisor mode with temperature
â”‚   â”œâ”€â”€ terminal_chat_v4.py # Chat with improved advisor mode
â”‚   â”œâ”€â”€ advice_mode.py      # Advisor mode logic
â”‚   â”œâ”€â”€ advice_session.py   # Session state management
â”‚   â”œâ”€â”€ temperature_utils.py # Temperature utilities
â”‚   â”œâ”€â”€ experiment_temperature.py # Temperature experiments
â”‚   â”œâ”€â”€ tests/             # Tests for all components
â”‚   â”œâ”€â”€ pyproject.toml     # Poetry dependencies
â”‚   â”œâ”€â”€ Makefile          # Development commands
â”‚   â””â”€â”€ README.md         # Project documentation
â”œâ”€â”€ day_05/               # Day 5 - Local models and message history
â”‚   â”œâ”€â”€ terminal_chat_v5.py # Chat with local model support
â”‚   â”œâ”€â”€ advice_mode_v5.py   # Advisor mode for local models
â”‚   â”œâ”€â”€ advice_session.py   # Session state management
â”‚   â”œâ”€â”€ temperature_utils.py # Temperature utilities
â”‚   â”œâ”€â”€ check_models.py     # Local model availability check
â”‚   â”œâ”€â”€ demo_*.py          # Demo scripts
â”‚   â”œâ”€â”€ tests/             # Tests for all components
â”‚   â”œâ”€â”€ requirements.txt   # Project dependencies
â”‚   â”œâ”€â”€ Makefile          # Development commands
â”‚   â””â”€â”€ README.md         # Project documentation
â”œâ”€â”€ day_06/               # Day 6 - Testing local models on logical puzzles
â”‚   â”œâ”€â”€ src/              # Testing system source code
â”‚   â”‚   â”œâ”€â”€ main.py       # Main testing module
â”‚   â”‚   â”œâ”€â”€ model_client.py # Local model client
â”‚   â”‚   â”œâ”€â”€ riddles.py    # Collection of logical puzzles
â”‚   â”‚   â”œâ”€â”€ report_generator.py # Report generator
â”‚   â”‚   â””â”€â”€ __init__.py   # Package initialization
â”‚   â”œâ”€â”€ tests/           # Tests for all components
â”‚   â”œâ”€â”€ pyproject.toml   # Poetry dependencies
â”‚   â”œâ”€â”€ Makefile         # Development commands
â”‚   â””â”€â”€ README.md        # Project documentation
â””â”€â”€ day_07/               # Day 7 - Multi-Agent System for Code Generation and Review
    â”œâ”€â”€ agents/          # System agents
    â”‚   â”œâ”€â”€ api/        # FastAPI services for agents
    â”‚   â”‚   â”œâ”€â”€ generator_api.py # Generator agent API
    â”‚   â”‚   â””â”€â”€ reviewer_api.py  # Reviewer agent API
    â”‚   â””â”€â”€ core/       # Agent core (generator, reviewer)
    â”‚       â”œâ”€â”€ base_agent.py # Base agent class
    â”‚       â”œâ”€â”€ code_generator.py # Code generator agent
    â”‚       â”œâ”€â”€ code_reviewer.py # Code reviewer agent
    â”‚       â””â”€â”€ model_client_adapter.py # Model integration
    â”œâ”€â”€ communication/   # Communication layer between agents
    â”‚   â”œâ”€â”€ agent_client.py # HTTP client with retry logic
    â”‚   â””â”€â”€ message_schema.py # Request/response models
    â”œâ”€â”€ prompts/        # Prompt templates for models
    â”‚   â”œâ”€â”€ generator_prompts.py # Generation prompts
    â”‚   â””â”€â”€ reviewer_prompts.py  # Review prompts
    â”œâ”€â”€ tests/          # System tests
    â”‚   â”œâ”€â”€ test_generator.py
    â”‚   â”œâ”€â”€ test_reviewer.py
    â”‚   â””â”€â”€ test_orchestrator.py
    â”œâ”€â”€ examples/       # Usage examples
    â”œâ”€â”€ orchestrator.py # Workflow orchestrator
    â”œâ”€â”€ main.py         # CLI interface
    â”œâ”€â”€ demo.py         # Demo examples
    â”œâ”€â”€ Dockerfile      # Multi-stage Docker image
    â”œâ”€â”€ docker-compose*.yml # Deployment configurations
    â”œâ”€â”€ README.md       # Project documentation (EN)
    â”œâ”€â”€ README.ru.md    # Project documentation (RU)
    â”œâ”€â”€ DEVELOPER_GUIDE.md # Developer guide (EN)
    â”œâ”€â”€ DEVELOPER_GUIDE.ru.md # Developer guide (RU)
    â”œâ”€â”€ ARCHITECTURE.md # System architecture (EN)
    â”œâ”€â”€ ARCHITECTURE.ru.md # System architecture (RU)
    â”œâ”€â”€ DEPLOYMENT.md   # Deployment guide (EN)
    â”œâ”€â”€ DEPLOYMENT.ru.md # Deployment guide (RU)
    â”œâ”€â”€ TROUBLESHOOTING.md # Troubleshooting guide (EN)
    â”œâ”€â”€ TROUBLESHOOTING.ru.md # Troubleshooting guide (RU)
    â”œâ”€â”€ pyproject.toml  # Poetry dependencies
    â”œâ”€â”€ Makefile        # Development commands
    â””â”€â”€ constants.py    # Configuration constants
â””â”€â”€ day_08/               # Day 8 - Enhanced Token Analysis System
    â”œâ”€â”€ core/           # Core business logic
    â”‚   â”œâ”€â”€ token_analyzer.py # Token counting strategies
    â”‚   â”œâ”€â”€ text_compressor.py # Text compression with strategy pattern
    â”‚   â”œâ”€â”€ ml_client.py # ML service client with retry logic
    â”‚   â”œâ”€â”€ experiments.py # Experiment management
    â”‚   â”œâ”€â”€ compressors/ # Compression strategy implementations
    â”‚   â”œâ”€â”€ factories/  # Factory pattern implementations
    â”‚   â”œâ”€â”€ builders/   # Builder pattern implementations
    â”‚   â””â”€â”€ validators/ # Request validation
    â”œâ”€â”€ domain/         # Domain layer (DDD)
    â”‚   â”œâ”€â”€ entities/   # Domain entities
    â”‚   â”œâ”€â”€ value_objects/ # Immutable value objects
    â”‚   â”œâ”€â”€ repositories/ # Repository interfaces
    â”‚   â””â”€â”€ services/   # Domain services
    â”œâ”€â”€ application/    # Application layer
    â”‚   â”œâ”€â”€ use_cases/  # Business use cases
    â”‚   â”œâ”€â”€ services/   # Application services
    â”‚   â””â”€â”€ dto/        # Data transfer objects
    â”œâ”€â”€ infrastructure/ # Infrastructure layer
    â”‚   â”œâ”€â”€ repositories/ # Repository implementations
    â”‚   â”œâ”€â”€ external/   # External service integrations
    â”‚   â””â”€â”€ config/     # Configuration management
    â”œâ”€â”€ tests/          # Comprehensive test suite
    â”‚   â”œâ”€â”€ integration/ # End-to-end tests
    â”‚   â”œâ”€â”€ regression/ # Baseline behavior tests
    â”‚   â”œâ”€â”€ performance/ # Performance baseline tests
    â”‚   â””â”€â”€ mocks/      # Test doubles
    â”œâ”€â”€ docs/           # Technical documentation
    â”‚   â”œâ”€â”€ DEVELOPMENT_GUIDE.md # Development practices
    â”‚   â”œâ”€â”€ DOMAIN_GUIDE.md # Domain-driven design
    â”‚   â”œâ”€â”€ ML_ENGINEERING.md # ML framework guide
    â”‚   â””â”€â”€ ASYNC_TESTING_BEST_PRACTICES.md # Testing guide
    â”œâ”€â”€ examples/       # Usage examples
    â”œâ”€â”€ reports/        # Demo and analysis reports
    â”œâ”€â”€ demo_enhanced.py # Enhanced demonstration
    â”œâ”€â”€ demo.py         # Basic demonstration
    â”œâ”€â”€ README.md       # Comprehensive documentation (EN)
    â”œâ”€â”€ README.ru.md    # Executive summary (RU)
    â”œâ”€â”€ architecture.md # System architecture
    â”œâ”€â”€ api.md          # API reference
    â”œâ”€â”€ TASK.md         # Original requirements
    â”œâ”€â”€ TASK_VERIFICATION_REPORT.md # Requirements verification
    â”œâ”€â”€ PROJECT_SUMMARY.md # Project achievements
    â”œâ”€â”€ pyproject.toml  # Poetry dependencies
    â”œâ”€â”€ Makefile        # Development commands
    â””â”€â”€ pytest.ini      # Test configuration
```

## ğŸš€ Quick Start

### 1. Setup API Keys

```bash
make setup  # Creates api_key.txt from template
# Add your API keys to api_key.txt:
# perplexity:your_perplexity_key
# chadgpt:your_chadgpt_key
```

### 2. Start Local Models (Optional)

```bash
# Start all local models
cd local_models
docker-compose up -d

# Check availability
curl http://localhost:8000/chat  # Qwen
curl http://localhost:8001/chat  # Mistral
curl http://localhost:8002/chat  # TinyLlama
```

### 3. Choose a Project

```bash
# Day 1 - Simple chat
cd day_01
make install
make chat

# Day 2 - Improved chat with JSON responses
cd ../day_02
make install
make chat

# Day 3 - Advisor mode with model constraints
cd ../day_03
make install
make run

# Day 4 - Improved advisor mode with temperature
cd ../day_04
make install
make run

# Day 5 - Local models and message history
cd ../day_05
make install
make run

# Day 6 - Testing local models on logical puzzles
cd ../day_06
make install
make run

# Day 7 - Multi-Agent System for Code Generation and Review
cd ../day_07

# Start StarCoder (required)
cd ../local_models
docker-compose up -d starcoder-chat

# Start agents via Docker Compose
cd ../day_07
make start-bridge  # Simple startup
# or
make start-traefik # With Traefik reverse proxy

# CLI usage
make demo          # Run demo
make run-simple    # Simple code generation

# Day 8 - Enhanced Token Analysis System
cd ../day_08

# Install dependencies
make install-dev

# Run comprehensive tests
make test

# Run demonstrations
make demo          # Basic demo
make demo-enhanced # Enhanced demo with reports
python examples/task_demonstration.py # TASK.md verification
```

## ğŸ“Š Project Comparison

| Project | Complexity | Technologies | Models | Features |
|---------|-----------|--------------|--------|----------|
| day_01 | â­ | Python, API | Perplexity | Simple chat |
| day_02 | â­â­ | Python, JSON | Perplexity | JSON responses |
| day_03 | â­â­ | Python, State | Perplexity | Advisor mode |
| day_04 | â­â­â­ | Python, Temperature | Perplexity | Temperature experiments |
| day_05 | â­â­â­ | Python, SDK, Docker | Local | SDK integration |
| day_06 | â­â­â­â­ | Python, SDK, Testing | Local | Model testing |
| day_07 | â­â­â­â­â­ | FastAPI, Docker, Traefik | 4 models | Multi-Agent System |
| day_08 | â­â­â­â­â­ | Clean Architecture, DDD, ML Engineering | 4 models | Token Analysis & Compression |

## ğŸ“š Project Descriptions

### Local Models - Local Language Model Infrastructure

ğŸ  **Shared module** for working with local language models. Provides a unified API for various models and can be used by any project in the repository.

**Technologies:**
- FastAPI (API server)
- Docker Compose (orchestration)
- HuggingFace Transformers
- NVIDIA CUDA (GPU acceleration)
- 4-bit quantization (memory efficiency)

**Supported Models:**
- **Qwen-4B** (port 8000) - Fast responses, ~8GB RAM
- **Mistral-7B** (port 8001) - High quality, ~14GB RAM  
- **TinyLlama-1.1B** (port 8002) - Compact, ~4GB RAM

**Key Features:**
- ğŸ”„ **Unified API**: Standard OpenAI-compatible interface
- ğŸ³ **Docker Orchestration**: Automatic container management
- ğŸ¯ **Auto-formatting**: Support for different prompt formats
- âš¡ **GPU Acceleration**: Automatic NVIDIA GPU usage
- ğŸ”’ **Privacy**: Data never leaves local machine
- ğŸ’° **Cost Savings**: No external API costs

**Quick Start:**
```bash
cd local_models
docker-compose up -d
curl http://localhost:8000/chat  # Test Qwen
```

**Integration:**
Used in `day_05/` and can be easily connected to any future projects.

### Shared SDK - Unified Model Interaction

ğŸ› ï¸ **Unified SDK** for working with various language models. Provides a consistent interface regardless of the model provider (Perplexity, ChatGPT, or local models).

**Key Features:**
- ğŸ”„ **Unified Interface**: Same API for all models
- ğŸ¯ **Provider Abstraction**: Easy switching between providers
- ğŸ“Š **Usage Statistics**: Token counting and cost tracking
- ğŸ”§ **Configuration Management**: Centralized settings
- ğŸ›¡ï¸ **Error Handling**: Standardized exception handling
- ğŸ“ˆ **Performance Monitoring**: Response time tracking

**Supported Providers:**
- **Perplexity**: High-quality responses
- **ChatGPT**: OpenAI's flagship model
- **Local Models**: Qwen, Mistral, TinyLlama

**Usage:**
```python
from shared.clients.model_client import ModelClient

client = ModelClient(provider="perplexity")
response = await client.chat("Hello, world!")
```

**Integration:**
Used in `day_05/` and `day_06/` for model interaction.

### Day 01 - Terminal Chat with AI

**Purpose**: Introduction to AI-powered terminal chat

**Features:**
- Simple terminal interface
- Real-time chat with AI
- Basic error handling
- Clean exit functionality

**Technologies:**
- Python 3.10+
- Perplexity API
- Terminal interface

**Quick Start:**
```bash
cd day_01
make install
make chat
```

### Day 02 - Improved Chat with JSON Responses

**Purpose**: Enhanced chat with structured JSON responses

**Features:**
- JSON-formatted responses
- Better error handling
- Improved user experience
- Response validation

**Technologies:**
- Python 3.10+
- Perplexity API
- JSON processing
- Pydantic validation

**Quick Start:**
```bash
cd day_02
make install
make chat
```

### Day 03 - Advisor Mode with Model Constraints

**Purpose**: Structured dialogue with AI advisor mode

**Features:**
- Advisor mode with constraints
- Session state management
- Context preservation
- Structured responses

**Technologies:**
- Python 3.10+
- Perplexity API
- State management
- Context handling

**Quick Start:**
```bash
cd day_03
make install
make run
```

### Day 04 - Improved Advisor Mode with Temperature

**Purpose**: Enhanced advisor mode with temperature control

**Features:**
- Temperature-based response variation
- Advanced advisor mode
- Experimentation tools
- Performance metrics

**Technologies:**
- Python 3.10+
- Perplexity API
- Temperature utilities
- Experimentation framework

**Quick Start:**
```bash
cd day_04
make install
make run
```

### Day 05 - Local Models and Message History

**Purpose**: Integration with local models and message history

**Features:**
- Local model support
- Message history
- Unified API
- SDK integration

**Technologies:**
- Python 3.10+
- Shared SDK
- Local models
- Docker integration

**Quick Start:**
```bash
cd day_05
make install
make run
```

### Day 06 - Testing Local Models on Logical Puzzles

**Purpose**: Comprehensive testing of local models on logical puzzles

**Features:**
- Logical puzzle testing
- Model comparison
- Report generation
- Performance analysis

**Technologies:**
- Python 3.10+
- Shared SDK
- Testing framework
- Report generation

**Quick Start:**
```bash
cd day_06
make install
make run
```

### Day 07 - Multi-Agent System for Code Generation and Review

ğŸ¤– **Professional system** for automated Python code generation and review using specialized AI agents. Supports multiple language models (StarCoder, Mistral, Qwen, TinyLlama).

#### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestrator  â”‚    â”‚  Code Generator â”‚    â”‚  Code Reviewer  â”‚
â”‚                 â”‚    â”‚     Agent       â”‚    â”‚     Agent       â”‚
â”‚  - Coordinates  â”‚â—„â”€â”€â–ºâ”‚  - Generates    â”‚â—„â”€â”€â–ºâ”‚  - Reviews      â”‚
â”‚  - Manages      â”‚    â”‚  - Creates      â”‚    â”‚  - Analyzes     â”‚
â”‚  - Saves        â”‚    â”‚  - Validates    â”‚    â”‚  - Scores       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Shared SDK     â”‚
                    â”‚  - StarCoder    â”‚
                    â”‚  - Mistral      â”‚
                    â”‚  - Qwen         â”‚
                    â”‚  - TinyLlama    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Technologies
- FastAPI (REST API for agents)
- Docker Compose (service orchestration)
- Traefik (reverse proxy and load balancer)
- Poetry (dependency management)
- Shared SDK (unified model interaction)
- Pydantic (data validation)
- pytest (testing)

#### Supported Models
- **StarCoder-7B** (default) - Specialized for code generation
- **Mistral-7B** - High quality, general-purpose model
- **Qwen-4B** - Fast responses, good quality
- **TinyLlama-1.1B** - Compact and fast

#### Key Components

**1. Code Generator Agent (port 9001)**
- Generates Python functions from descriptions
- Creates comprehensive tests
- Validates generated code
- Supports code refinement
- API endpoints: /generate, /refine, /validate

**2. Code Reviewer Agent (port 9002)**
- Analyzes code quality
- Checks PEP8 compliance
- Evaluates test coverage
- Calculates code complexity
- API endpoints: /review, /analyze-pep8, /calculate-complexity

**3. Orchestrator**
- Coordinates workflow between agents
- Manages task processing
- Saves results
- Collects statistics
- Handles errors with retry logic

**4. Communication Layer**
- HTTP client with retry logic
- Exponential backoff for resilience
- Pydantic models for validation
- Structured requests/responses

#### Key Features

- âœ¨ **Multi-Model Support**: Choice of 4 language models
- ğŸ¤– **Specialized Agents**: Separate agents for generation and review
- ğŸ”„ **Workflow Orchestration**: Automatic agent coordination
- ğŸ“Š **Quality Metrics**: PEP8, test coverage, complexity
- ğŸ³ **Docker Deployment**: Multiple deployment options
- ğŸ” **Health Monitoring**: Endpoints for status checks
- ğŸ“ˆ **Statistics**: Detailed agent performance metrics
- ğŸ›¡ï¸ **Error Handling**: Retry logic with exponential backoff
- ğŸ“ **Comprehensive Docs**: 5 documents including architecture and deployment
- ğŸ”’ **Security**: Multi-stage Docker builds, non-root users

#### Deployment Options

**Bridge Network (simple)**:
```bash
make start-bridge
```
- Generator: http://localhost:9001
- Reviewer: http://localhost:9002

**Traefik Reverse Proxy (production)**:
```bash
make start-traefik
```
- Generator: http://generator.localhost
- Reviewer: http://reviewer.localhost
- Traefik Dashboard: http://localhost:8080

#### Usage Examples

**CLI Generation**:
```bash
python main.py "Create a function to calculate fibonacci numbers"
```

**Python API**:
```python
from orchestrator import MultiAgentOrchestrator
from communication.message_schema import OrchestratorRequest

orchestrator = MultiAgentOrchestrator()

# Simple generation
request = OrchestratorRequest(
    task_description="Create a REST API endpoint",
    model_name="starcoder"
)
result = await orchestrator.process_task(request)

# Different models for generation and review
request = OrchestratorRequest(
    task_description="Create a data processing pipeline",
    model_name="starcoder",
    reviewer_model_name="mistral"
)
result = await orchestrator.process_task(request)
```

## Performance Optimization

### StarCoder2 GPTQ Optimization (Recommended for RTX 3070 Ti)

The system uses optimized StarCoder2-7B-GPTQ variant with:
- 4-bit GPTQ quantization (~6-7GB VRAM)
- FlashAttention for 2-3x faster inference
- BFloat16 precision
- Optimized generation parameters

**Expected Performance**:
- VRAM usage: 6-7GB (down from 14GB)
- Inference speed: 2-3x faster
- Quality: ~95% of full precision model

**Dependencies**:
- `auto-gptq`: GPTQ model loading
- `flash-attn`: Optimized attention mechanism
- `optimum`: Hugging Face optimizations

**Installation Notes**:
```bash
# flash-attn requires CUDA toolkit and may take 5-10 minutes to compile
pip install flash-attn --no-build-isolation
```

#### Workflow Process

1. **Task Submission**: User submits task description
2. **Code Generation**: Generator Agent creates code and tests
3. **Code Review**: Reviewer Agent analyzes quality
4. **Results Aggregation**: Orchestrator collects results
5. **Persistence**: Results saved to JSON
6. **Statistics Update**: Performance metrics updated

#### API Endpoints

**Generator Agent (9001)**:
- POST /generate - code generation
- POST /refine - code improvement
- POST /validate - code validation
- GET /health - health check
- GET /stats - performance statistics

**Reviewer Agent (9002)**:
- POST /review - full code review
- POST /analyze-pep8 - PEP8 analysis
- POST /analyze-test-coverage - coverage analysis
- POST /calculate-complexity - complexity calculation
- GET /health - health check
- GET /stats - performance statistics

#### Testing

```bash
# All tests
make test

# With coverage
make test-coverage

# Unit tests only
make test-unit

# Integration tests only
make test-integration
```

#### Documentation

The project includes a complete documentation set:

- **README.md** - Main documentation and quick start
- **DEVELOPER_GUIDE.md** - Developer guide
- **ARCHITECTURE.md** - Detailed system architecture
- **DEPLOYMENT.md** - Deployment guide
- **TROUBLESHOOTING.md** - Troubleshooting guide
- **API.md** - API endpoint documentation

#### SDK Integration

Uses `shared/` SDK for unified model interaction:
- Unified interface for all models
- Automatic configuration
- Error handling
- Retry logic

#### Performance

**Typical generation time**:
- StarCoder: 5-10 seconds
- Mistral: 6-12 seconds
- Qwen: 3-8 seconds
- TinyLlama: 2-5 seconds

**Resource requirements**:
- CPU: 4+ cores (recommended)
- RAM: 16GB+ (32GB for StarCoder)
- GPU: NVIDIA with 12GB+ VRAM (for StarCoder)
- Disk: 20GB+ (for models)

#### Scaling

Supports horizontal scaling:
```bash
docker-compose up -d --scale generator-agent=3 --scale reviewer-agent=2
```

#### Security

- Multi-stage Docker builds for minimal size
- Non-root users in containers
- Health checks for monitoring
- Resource limits for all services
- Traefik for secure routing

#### Monitoring

- Health endpoints for status checks
- Agent performance statistics
- All workflow results saved
- Comprehensive logging

#### Production Ready

- âœ… Comprehensive documentation
- âœ… Unit and integration tests
- âœ… Error handling with retries
- âœ… Health monitoring
- âœ… Docker multi-stage builds
- âœ… Resource management
- âœ… Security best practices
- âœ… Logging and metrics

#### Next Steps

1. Study documentation in `day_07/DEVELOPER_GUIDE.md`
2. Run demo to understand workflow
3. Experiment with different models
4. Integrate into your own projects
5. Extend functionality with new agents

### Day 08 - Enhanced Token Analysis System

ğŸ¯ **Production-ready system** for token analysis, compression, and ML model interaction. Features Clean Architecture with Domain-Driven Design, comprehensive testing, and ML Engineering framework.

#### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Day 08 Enhanced System                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Demo      â”‚  â”‚   Enhanced  â”‚  â”‚  SDK        â”‚            â”‚
â”‚  â”‚  Scripts    â”‚  â”‚  Features   â”‚  â”‚  Adapters   â”‚            â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚            â”‚
â”‚  â”‚ â€¢ Enhanced  â”‚  â”‚ â€¢ Model     â”‚  â”‚ â€¢ Generator â”‚            â”‚
â”‚  â”‚ â€¢ Model     â”‚  â”‚   Switching â”‚  â”‚ â€¢ Reviewer  â”‚            â”‚
â”‚  â”‚   Switching â”‚  â”‚ â€¢ Quality   â”‚  â”‚ â€¢ Direct    â”‚            â”‚
â”‚  â”‚ â€¢ Reports   â”‚  â”‚   Analysis  â”‚  â”‚ â€¢ REST      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                 â”‚                 â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Core Day 08 Components                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚   Token     â”‚  â”‚   Text      â”‚  â”‚   ML        â”‚        â”‚ â”‚
â”‚  â”‚  â”‚   Counter   â”‚  â”‚  Compressor â”‚  â”‚   Client    â”‚        â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Simple    â”‚  â”‚ â€¢ Strategy  â”‚  â”‚ â€¢ Retry     â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Accurate  â”‚  â”‚ â€¢ Template  â”‚  â”‚ â€¢ Circuit   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Hybrid    â”‚  â”‚ â€¢ Factory   â”‚  â”‚ â€¢ Breaker   â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚   Model     â”‚  â”‚   Token     â”‚  â”‚ Compression â”‚        â”‚ â”‚
â”‚  â”‚  â”‚   Switcher  â”‚  â”‚   Limit    â”‚  â”‚ Evaluator   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚   Tester   â”‚  â”‚             â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ SDK       â”‚  â”‚ â€¢ Three-   â”‚  â”‚ â€¢ All       â”‚        â”‚ â”‚
â”‚  â”‚  â”‚   Workflow  â”‚  â”‚   Stage    â”‚  â”‚   Algorithmsâ”‚        â”‚ â”‚
â”‚  â”‚  â”‚ â€¢ Quality   â”‚  â”‚ â€¢ Dynamic  â”‚  â”‚ â€¢ Quality   â”‚        â”‚ â”‚
â”‚  â”‚  â”‚   Analysis  â”‚  â”‚ â€¢ Model-   â”‚  â”‚ â€¢ Performanceâ”‚       â”‚ â”‚
â”‚  â”‚  â”‚             â”‚  â”‚   Specific â”‚  â”‚             â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Technologies

- **Clean Architecture**: Domain, Application, Infrastructure, Presentation layers
- **Domain-Driven Design**: Entities, value objects, repositories, domain services
- **SOLID Principles**: Single responsibility, dependency injection, interface segregation
- **Design Patterns**: Strategy, Factory, Builder, Template Method, Circuit Breaker, Facade
- **ML Engineering**: Model evaluation, performance monitoring, experiment tracking, model registry
- **Comprehensive Testing**: 282 tests with 74% coverage, integration, regression, performance tests
- **Type Safety**: 100% type hints coverage in core modules
- **Quality Assurance**: Strict linting, pre-commit hooks, security scanning

#### Supported Models

- **StarCoder-7B** (default) - Specialized for code generation
- **Mistral-7B** - High quality, general-purpose model
- **Qwen-4B** - Fast responses, good quality
- **TinyLlama-1.1B** - Compact and fast

#### Key Components

**1. Token Analysis (`core/token_analyzer.py`)**
- Multiple counting strategies (simple estimation, ML-based, hybrid)
- Model-specific limits and validation
- Input/output token counting for requests and responses
- Support for all models with accurate token estimation

**2. Text Compression (`core/text_compressor.py`)**
- Strategy pattern with 5 compression algorithms
- Truncation, keywords, extractive, semantic, summarization
- Automatic compression on limit-exceeding queries
- Compression ratio calculation and quality analysis

**3. ML Client (`core/ml_client.py`)**
- Resilient client with retry logic and circuit breaker
- Request validation and error handling
- Performance monitoring and statistics
- Integration with SDK agents

**4. Experiments (`core/experiments.py`)**
- Builder pattern for experiment result construction
- Comprehensive experiment tracking and management
- Model comparison and analysis
- Structured logging and reporting

#### Key Features

- âœ¨ **Token Analysis**: Accurate token counting with multiple strategies
- ğŸ—œï¸ **Text Compression**: 5 compression algorithms with quality evaluation
- ğŸ—ï¸ **Clean Architecture**: Domain-driven design with SOLID principles
- ğŸ§ª **Comprehensive Testing**: 282 tests with excellent coverage
- ğŸ“Š **ML Engineering**: Production-ready MLOps framework
- ğŸ”„ **SDK Integration**: Unified agent system integration
- ğŸ“ˆ **Performance Monitoring**: Detailed metrics and analytics
- ğŸ›¡ï¸ **Error Handling**: Robust exception management and retry logic
- ğŸ“ **Documentation**: 12+ comprehensive guides and examples
- ğŸ”’ **Security**: Input validation and security scanning

#### TASK.md Requirements Fulfillment

âœ… **Requirement 1: Token Counting**
- Implementation: `core/token_analyzer.py`
- Features: Input/output token counting, multiple strategies, model limits
- Verification: All models tested with accurate token counts

âœ… **Requirement 2: Query Comparison**
- Implementation: `core/token_limit_tester.py`
- Features: Three-stage testing (short/medium/long queries)
- Verification: Comprehensive query analysis and behavior documentation

âœ… **Requirement 3: Text Compression**
- Implementation: `core/text_compressor.py` with strategy pattern
- Features: 5 compression algorithms, automatic application
- Verification: Compression applied to limit-exceeding queries

#### Performance Metrics

- **Token counting**: ~0.1ms per 1000 characters
- **Text compression**: ~5ms per 10KB text
- **ML requests**: ~200ms average response time
- **Memory usage**: ~50MB baseline
- **Test execution**: 282 tests in ~4.6 seconds
- **Code coverage**: 74% with comprehensive test suite

#### Production Ready Features

- âœ… **Robust Error Handling**: Comprehensive exception hierarchy
- âœ… **Type Safety**: 100% type hints in core modules
- âœ… **Security**: Input validation and sanitization
- âœ… **Monitoring**: Performance metrics and health checks
- âœ… **Documentation**: Complete API reference and guides
- âœ… **Testing**: Unit, integration, regression, performance tests
- âœ… **Quality**: PEP8 compliance, linting, pre-commit hooks
- âœ… **Architecture**: Clean Architecture with DDD patterns

#### Usage Examples

**Basic Token Counting:**
```python
from core.token_analyzer import SimpleTokenCounter
from tests.mocks.mock_config import MockConfiguration

config = MockConfiguration()
counter = SimpleTokenCounter(config=config)
token_info = counter.count_tokens("Hello world", "starcoder")
print(f"Tokens: {token_info.count}")
```

**Text Compression:**
```python
from core.text_compressor import SimpleTextCompressor

compressor = SimpleTextCompressor(token_counter)
result = compressor.compress_text(
    text="Very long text...",
    max_tokens=1000,
    model_name="starcoder",
    strategy="truncation"
)
print(f"Compression ratio: {result.compression_ratio}")
```

**Running Experiments:**
```python
from core.experiments import TokenLimitExperiments

experiments = TokenLimitExperiments(ml_client, token_counter, compressor)
results = await experiments.run_limit_exceeded_experiment("starcoder")
```

#### Next Steps

1. Study comprehensive documentation in `day_08/README.md`
2. Run `make demo` to see token analysis in action
3. Explore `examples/task_demonstration.py` for TASK.md verification
4. Review `docs/` directory for technical guides
5. Integrate token analysis into your own projects
6. Extend compression strategies for specific use cases

## ğŸ› ï¸ Technologies and Dependencies

### Core Technologies
- **Python 3.10+**: Main programming language
- **Poetry**: Dependency management
- **Docker**: Containerization
- **Docker Compose**: Service orchestration
- **FastAPI**: Web framework for APIs
- **Pydantic**: Data validation
- **pytest**: Testing framework

### AI/ML Technologies
- **HuggingFace Transformers**: Model integration
- **NVIDIA CUDA**: GPU acceleration
- **4-bit Quantization**: Memory optimization
- **Local Models**: Qwen, Mistral, TinyLlama, StarCoder
- **Token Analysis**: Advanced token counting and compression strategies
- **ML Engineering**: Model evaluation, monitoring, experiment tracking

### Architecture & Design Technologies
- **Clean Architecture**: Domain, Application, Infrastructure layers
- **Domain-Driven Design**: Entities, value objects, repositories
- **SOLID Principles**: Design patterns and best practices
- **Design Patterns**: Strategy, Factory, Builder, Template Method, Circuit Breaker
- **Type Safety**: Comprehensive type hints and validation

### Infrastructure Technologies
- **Traefik**: Reverse proxy and load balancer
- **NVIDIA Container Toolkit**: GPU support
- **Multi-stage Docker builds**: Security and optimization

## ğŸ¤ Contributing

We welcome contributions! Please see individual project README files for specific contribution guidelines.

### General Guidelines
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

### Code Standards
- Follow PEP 8 for Python code
- Use type hints
- Write comprehensive tests
- Document your changes
- Follow existing patterns

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- HuggingFace for model hosting and transformers library
- OpenAI for API inspiration
- The open-source community for tools and libraries
- Contributors and users of this project

---

**Note**: This is a learning project exploring AI and language models. Use responsibly and in accordance with applicable terms of service.