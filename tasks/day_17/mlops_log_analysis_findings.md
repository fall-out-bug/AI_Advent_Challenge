# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ —Å—Ç—É–¥–µ–Ω—Ç–∞: –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

## –û–±–∑–æ—Ä

–ê–Ω–∞–ª–∏–∑ 7 –ª–æ–≥-—Ñ–∞–π–ª–æ–≤ –ø–æ–∫–∞–∑–∞–ª **–∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É**, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞–ø—É—Å–∫ Airflow, –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π. –í—Å–µ –ø—Ä–æ–±–ª–µ–º—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–µ—à–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏.

---

## üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞: PermissionError –≤ Airflow

### –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –ø—Ä–æ–±–ª–µ–º—ã

```
PermissionError: [Errno 13] Permission denied: '/opt/airflow/logs/scheduler'
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler/2025-11-03'
```

### –ì–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è

- **–§–∞–π–ª**: `airflow.log`, `run_stderr.txt`
- **–ü–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è**: 8+ —Ä–∞–∑
- **–í—Ä–µ–º—è**: 2025-11-03T20:36:40 - 2025-11-03T20:36:41

### –ö–æ—Ä–Ω–µ–≤–∞—è –ø—Ä–∏—á–∏–Ω–∞

1. **–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞**: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è `/opt/airflow/logs/scheduler` –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
2. **–í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞**: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å `airflow` (–∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å) –Ω–µ –∏–º–µ–µ—Ç –ø—Ä–∞–≤ –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
3. **–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏–µ**: Airflow –≤–æ–æ–±—â–µ –Ω–µ –º–æ–∂–µ—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ –º–æ–∂–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### –°—Ç–µ–∫-—Ç—Ä–µ–π—Å (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)

```
pathlib.py:1116 in mkdir
    os.mkdir(self, mode)
            ‚Üì
FileNotFoundError: [Errno 2] No such file or directory

‚Üì (–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è)

file_processor_handler.py:53 in __init__
    Path(self._get_log_directory()).mkdir(parents=True, exist_ok=True)
            ‚Üì
PermissionError: [Errno 13] Permission denied: '/opt/airflow/logs/scheduler'

‚Üì (propagation)

settings.py:531 in initialize
    LOGGING_CLASS_PATH = configure_logging()
            ‚Üì
ValueError: Unable to configure handler 'processor'
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é

#### –†–µ—à–µ–Ω–∏–µ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ Dockerfile (‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```dockerfile
# –í Dockerfile Airflow –¥–æ–±–∞–≤–∏—Ç—å:

FROM apache/airflow:2.9.0-python3.11

# ... –¥—Ä—É–≥–∏–µ —Å–ª–æ–∏ ...

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ª–æ–≥–æ–≤ –î–û –∑–∞–ø—É—Å–∫–∞
RUN mkdir -p /opt/airflow/logs && \
    mkdir -p /opt/airflow/logs/scheduler && \
    chown -R airflow:0 /opt/airflow/logs && \
    chmod -R 755 /opt/airflow/logs

# –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è data —Ç–∞–∫–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
RUN mkdir -p /opt/airflow/data && \
    chown -R airflow:0 /opt/airflow/data

ENTRYPOINT ["/opt/airflow/entrypoint.sh"]
```

#### –†–µ—à–µ–Ω–∏–µ 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Init –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–≤ docker-compose)

```yaml
services:
  airflow-init:
    image: apache/airflow:2.9.0-python3.11
    entrypoint: >
      bash -c "
      mkdir -p /opt/airflow/logs/scheduler &&
      mkdir -p /opt/airflow/data &&
      chown -R airflow:0 /opt/airflow/logs /opt/airflow/data &&
      chmod -R 755 /opt/airflow/logs /opt/airflow/data
      "
    volumes:
      - airflow_logs:/opt/airflow/logs
      - airflow_data:/opt/airflow/data
    profiles:
      - init

  airflow:
    image: custom-airflow:latest
    depends_on:
      - airflow-init
    volumes:
      - airflow_logs:/opt/airflow/logs
      - airflow_data:/opt/airflow/data
    # ... rest config ...

volumes:
  airflow_logs:
  airflow_data:
```

#### –†–µ—à–µ–Ω–∏–µ 3: –ò—Å–ø—Ä–∞–≤–∏—Ç—å entrypoint.sh

```bash
#!/bin/bash

# airflow/entrypoint.sh

set -e

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
echo "Initializing Airflow directories..."
mkdir -p /opt/airflow/logs/scheduler
mkdir -p /opt/airflow/data
mkdir -p /opt/airflow/dags
mkdir -p /opt/airflow/plugins

# –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞
chown -R airflow:0 /opt/airflow/logs
chown -R airflow:0 /opt/airflow/data

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ë–î
airflow db init

# –ó–∞–ø—É—Å—Ç–∏—Ç—å Airflow
exec airflow webserver
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—à–µ–Ω–∏—è

–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è:

```bash
# 1. –ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å –æ–±—Ä–∞–∑
docker-compose build airflow

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker-compose up airflow

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker-compose logs airflow

# 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
docker-compose exec airflow ls -la /opt/airflow/logs/

# 5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞
docker-compose exec airflow ls -la /opt/airflow/ | grep logs
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: Airflow —Å—Ç–∞—Ä—Ç—É–µ—Ç —É—Å–ø–µ—à–Ω–æ –±–µ–∑ –æ—à–∏–±–æ–∫ PermissionError

---

## üü° –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: Native Hadoop Library

### –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –ø—Ä–æ–±–ª–µ–º—ã

```
WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```

### –ì–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è

- **–§–∞–π–ª—ã**: `spark-master.log`, `spark-worker-1.log`
- **–ü–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è**: 2 —Ä–∞–∑–∞ (–Ω–∞ master –∏ worker)
- **–£—Ä–æ–≤–µ–Ω—å**: WARNING (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)

### –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è

- Spark –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ ~5-15% –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —á–∏—Å—Ç—É—é Java —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –≤–º–µ—Å—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ native –∫–æ–¥–∞
- –ù–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–∏, –Ω–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∞

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

#### –†–µ—à–µ–Ω–∏–µ 1: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Hadoop native libraries (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞)

```dockerfile
FROM apache/spark:3.5.1-python3

RUN apt-get update && apt-get install -y \
    libhadoop-java \
    hadoop-native \
    && rm -rf /var/lib/apt/lists/*

# –ó–∞–¥–∞—Ç—åÁéØÂ¢É–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è Hadoop native
ENV HADOOP_HOME=/usr/lib/hadoop
ENV LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native:$LD_LIBRARY_PATH
```

#### –†–µ—à–µ–Ω–∏–µ 2: –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å warning (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)

```bash
# –î–æ–±–∞–≤–∏—Ç—å –≤ spark-defaults.conf
spark.driver.extraJavaOptions=-Dorg.apache.hadoop.hive.metastore.uris=
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:MaxGCPauseMillis=53
```

#### –†–µ—à–µ–Ω–∏–µ 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Spark —Å –ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ native libraries

```yaml
services:
  spark-master:
    image: docker.io/bitnami/spark:latest  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞–∑ —Å native libs
    # –≤–º–µ—Å—Ç–æ apache/spark:3.5.1-python3
```

---

## ‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è (OK)

### Redis

‚úÖ **–°—Ç–∞—Ç—É—Å**: OK  
–£—Å–ø–µ—à–Ω–æ —Å—Ç–∞—Ä—Ç–æ–≤–∞–ª –Ω–∞ –ø–æ—Ä—Ç—É 6379:

```
Redis version=7.2.12, bits=64, pid=1
Running mode=standalone, port=6379
Ready to accept connections tcp
```

**–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è default credentials (minioadmin:minioadmin)  
**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–æ–ª—å –≤ production

### MinIO

‚úÖ **–°—Ç–∞—Ç—É—Å**: OK, –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏

```
MinIO Object Storage Server RELEASE.2024-06-13
API: http://172.19.0.3:9000
WebUI: http://172.19.0.3:9001
Status: 1 Online, 0 Offline
```

**–ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã**:

1. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è default credentials: `minioadmin:minioadmin`
2. Standard parity —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤ 0 (–º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–æ—Ç–µ—Ä–µ –¥–∞–Ω–Ω—ã—Ö)
3. –í–µ—Ä—Å–∏—è MinIO –æ—Ç –∏—é–Ω—è 2024, –µ—Å—Ç—å –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ –≤–µ—Ä—Å–∏–∏

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**:

```yaml
# docker-compose.yml (–æ–±–Ω–æ–≤–∏—Ç—å)
services:
  minio:
    image: minio/minio:RELEASE.2025-01-14T23-27-41Z  # Latest version
    environment:
      MINIO_ROOT_USER: ${MINIO_USER:-your_access_key}
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD:-your_secret_key}
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
```

### Spark Cluster

‚úÖ **–°—Ç–∞—Ç—É—Å**: OK

**Master**:
- Started on port 7077
- Running Spark version 3.5.1
- Master UI: http://194469fc5e66:8080

**Worker**:
- Successfully registered with master
- 6 cores, 46.1 GiB RAM available
- Worker UI: http://9f6318e33bd7:8081

---

## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–æ–≥–æ–≤

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –ü—Ä–æ–±–ª–µ–º—ã | –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å |
|-----------|--------|----------|------------|
| Airflow | ‚ùå FAILED | PermissionError (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ª–æ–≥–æ–≤) | üî¥ CRITICAL |
| Spark Master | ‚úÖ OK | Native library warning | üü° WARNING |
| Spark Worker | ‚úÖ OK | Native library warning | üü° WARNING |
| Redis | ‚úÖ OK | Default credentials | ‚ÑπÔ∏è INFO |
| MinIO | ‚úÖ OK | Old version, default creds | ‚ÑπÔ∏è INFO |

---

## üéØ –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π (Priority Order)

### –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ (Must Do)

1. ‚úÖ **–ò—Å–ø—Ä–∞–≤–∏—Ç—å PermissionError –≤ Airflow**
   - –î–æ–±–∞–≤–∏—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –≤ Dockerfile
   - **–í—Ä–µ–º—è**: ~10 –º–∏–Ω—É—Ç
   - **–†–∏—Å–∫**: –ù–∏–∑–∫–∏–π

2. ‚úÖ **–ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å Docker –æ–±—Ä–∞–∑—ã**
   - `docker-compose build`
   - **–í—Ä–µ–º—è**: ~5 –º–∏–Ω—É—Ç (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è cache)

3. ‚úÖ **–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã**
   - `docker-compose down && docker-compose up`
   - **–í—Ä–µ–º—è**: ~30 —Å–µ–∫—É–Ω–¥

### –í–∞–∂–Ω–æ (Should Do)

1. üìã **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ credentials**
   - –ò–∑–º–µ–Ω–∏—Ç—å default –ø–∞—Ä–æ–ª–∏ Redis –∏ MinIO
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
   - **–í—Ä–µ–º—è**: ~15 –º–∏–Ω—É—Ç

2. üìã **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Hadoop native libraries** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
   - –£–ª—É—á—à–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Spark –Ω–∞ 5-15%
   - **–í—Ä–µ–º—è**: ~15 –º–∏–Ω—É—Ç

### –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ (Nice To Have)

1. üìã **–û–±–Ω–æ–≤–∏—Ç—å –≤–µ—Ä—Å–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤**
   - MinIO –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏
   - Spark (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã –Ω–æ–≤—ã–µ features)
   - **–í—Ä–µ–º—è**: ~20 –º–∏–Ω—É—Ç

---

## üìù –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞

### –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è PermissionError

**–û–∂–∏–¥–∞–µ–º—ã–π –ª–æ–≥:**
```
airflow-1 | ‚öôÔ∏è Initializing Airflow database...
airflow-1 | ‚öôÔ∏è Removing spark_default connection to avoid conflicts...
airflow-1 | üïì Waiting for Airflow webserver to be healthy...
airflow-1 | ‚úÖ Airflow webserver is healthy!
airflow-1 | Airflow started successfully on http://localhost:8080
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)

```bash
$ docker-compose exec airflow ls -la /opt/airflow/logs/

total 12
drwxr-xr-x 4 airflow root   4096 Nov  3 20:36:40 .
drwxr-xr-x 3 airflow root   4096 Nov  3 20:36:15 ..
drwxr-xr-x 2 airflow root   4096 Nov  3 20:36:40 scheduler
drwxr-xr-x 2 airflow root   4096 Nov  3 20:36:40 dag_processor_manager
```

---

## üîç –ö–∞–∫ –æ—Ç–ª–∞–¥–∏—Ç—å, –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è

### 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞

```bash
docker-compose logs -f airflow
docker-compose logs -f spark-master
```

### 2. –ó–∞–π—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å

```bash
docker-compose exec airflow bash

# –í–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞:
ls -la /opt/airflow/logs/
ls -la /opt/airflow/
whoami
id
```

### 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞

```bash
docker-compose exec airflow stat /opt/airflow/logs
# Output –¥–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å:
# Access: (0755/drwxr-xr-x)
# Uid: ( 1000/airflow)
# Gid: ( 0/root)
```

### 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å docker-compose volume mounts

```bash
docker volume ls | grep airflow
docker inspect workspace_airflow_logs  # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–æ—á–∫—É –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [Apache Airflow Docker Documentation](https://airflow.apache.org/docs/docker-stack/build.html)
- [Spark Python Docker Setup](https://spark.apache.org/docs/latest/running-on-kubernetes.html)
- [MinIO Security](https://min.io/docs/minio/linux/operations/secure-access-credentials.html)
- [Docker Volumes Best Practices](https://docs.docker.com/storage/volumes/)

---

## üìã –ß–µ–∫-–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—É—Å–∫–æ–º

- [ ] –û–±–Ω–æ–≤–ª–µ–Ω Dockerfile Airflow —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
- [ ] –ü–µ—Ä–µ—Å–æ–±—Ä–∞–Ω—ã –≤—Å–µ Docker –æ–±—Ä–∞–∑—ã (`docker-compose build`)
- [ ] –£–¥–∞–ª–µ–Ω—ã —Å—Ç–∞—Ä—ã–µ volumes (`docker volume rm workspace_airflow_logs`)
- [ ] Airflow —É—Å–ø–µ—à–Ω–æ —Å—Ç–∞—Ä—Ç—É–µ—Ç –±–µ–∑ –æ—à–∏–±–æ–∫
- [ ] –ú–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ Airflow UI –Ω–∞ http://localhost:8080
- [ ] –í—Å–µ DAGs –∑–∞–≥—Ä—É–∂–µ–Ω—ã
- [ ] Spark master –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:8080 (Spark UI)
- [ ] Redis —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø–æ—Ä—Ç—É 6379
- [ ] MinIO –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:9001

---

## üí° –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞

1. **–ò—Å–ø–æ–ª—å–∑—É–π .env —Ñ–∞–π–ª—ã** –¥–ª—è –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
2. **–í–µ—Ä—Å–∏–æ–Ω–∏—Ä—É–π Dockerfile'—ã** –≤–º–µ—Å—Ç–µ —Å –∫–æ–¥–æ–º
3. **–î–æ–±–∞–≤—å health checks** –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –≤ docker-compose.yml
4. **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è** –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (README.md)
5. **–ò—Å–ø–æ–ª—å–∑—É–π volumes** –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è persistence –¥–∞–Ω–Ω—ã—Ö

---

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞**: 2025-11-07  
**–í–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞**: 2.0 (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LLM)  
**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö**: 95%+
