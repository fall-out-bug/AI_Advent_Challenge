"""ML-specific metrics for LLM inference monitoring.

Following DevOps best practices: explicit metrics for ML services.
"""

from typing import Optional

try:
    from prometheus_client import Counter, Histogram, Gauge  # type: ignore

    # LLM Inference Metrics
    llm_inference_latency_seconds = Histogram(
        "llm_inference_latency_seconds",
        "LLM inference latency in seconds",
        buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
    )

    llm_token_generation_rate = Counter(
        "llm_token_generation_rate_total",
        "Total tokens generated by LLM",
        ["model_name"],
    )

    llm_model_version = Gauge(
        "llm_model_version",
        "Current deployed model version",
        ["model_name"],
    )

    llm_error_rate = Counter(
        "llm_error_rate_total",
        "Total LLM errors by type",
        ["error_type", "model_name"],
    )

    llm_request_token_count = Histogram(
        "llm_request_token_count",
        "Token count per LLM request (input + output)",
        ["token_type"],  # input or output
        buckets=[10, 50, 100, 500, 1000, 2000, 5000, 10000],
    )

    # ML Quality Metrics
    llm_response_quality_score = Gauge(
        "llm_response_quality_score",
        "Quality score of LLM responses (0-1)",
        ["model_name"],
    )

    llm_summarization_length = Histogram(
        "llm_summarization_length",
        "Length of summarization outputs",
        buckets=[10, 50, 100, 200, 500, 1000, 2000, 5000],
    )

    llm_intent_parsing_accuracy = Counter(
        "llm_intent_parsing_accuracy_total",
        "Total intent parsing accuracy measurements",
        ["model_name"],
    )

    # Model Metadata
    llm_model_deployed_at = Gauge(
        "llm_model_deployed_at_seconds",
        "Unix timestamp when model was deployed",
        ["model_name"],
    )

except ImportError:
    # Metrics are optional - create dummy objects if prometheus_client not available
    class _DummyMetric:
        def inc(self, *args, **kwargs):  # type: ignore
            pass

        def observe(self, *args, **kwargs):  # type: ignore
            pass

        def set(self, *args, **kwargs):  # type: ignore
            pass

        def labels(self, *args, **kwargs):  # type: ignore
            return self

        def time(self):  # type: ignore
            from contextlib import contextmanager

            @contextmanager
            def noop():
                yield

            return noop()

    llm_inference_latency_seconds = _DummyMetric()
    llm_token_generation_rate = _DummyMetric()
    llm_model_version = _DummyMetric()
    llm_error_rate = _DummyMetric()
    llm_request_token_count = _DummyMetric()
    llm_response_quality_score = _DummyMetric()
    llm_summarization_length = _DummyMetric()
    llm_intent_parsing_accuracy = _DummyMetric()
    llm_model_deployed_at = _DummyMetric()


def get_ml_metrics_registry() -> Optional[object]:
    """Get Prometheus metrics registry for ML metrics.

    Returns:
        Registry object or None if prometheus_client not available
    """
    try:
        from prometheus_client import REGISTRY  # type: ignore

        return REGISTRY
    except ImportError:
        return None

