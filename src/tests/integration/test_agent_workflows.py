"""Integration tests for agent workflows.

Following TDD principles and the Zen of Python:
- Simple is better than complex
- Readability counts
"""

import pytest
from unittest.mock import AsyncMock, MagicMock
from datetime import datetime

from src.application.orchestrators.multi_agent_orchestrator import (
    MultiAgentOrchestrator,
)
from src.domain.agents.code_generator import CodeGeneratorAgent
from src.domain.agents.code_reviewer import CodeReviewerAgent
from src.domain.messaging.message_schema import (
    CodeGenerationRequest,
    CodeGenerationResponse,
    CodeQualityMetrics,
    CodeReviewRequest,
    CodeReviewResponse,
    OrchestratorRequest,
    TaskMetadata,
)


class TestCodeGenerationToReviewWorkflow:
    """Test code generation to review workflow."""

    @pytest.mark.asyncio
    async def test_full_generation_to_review_workflow(self):
        """Test complete workflow from generation to review."""
        # Setup
        mock_generator = AsyncMock(spec=CodeGeneratorAgent)
        mock_reviewer = AsyncMock(spec=CodeReviewerAgent)

        gen_response = CodeGenerationResponse(
            task_description="Test task",
            generated_code="def hello(): print('Hello')",
            tests="def test_hello(): assert hello() is None",
            metadata=TaskMetadata(),
            generation_time=datetime.now(),
            tokens_used=100,
        )

        review_metrics = CodeQualityMetrics(
            pep8_compliance=True,
            pep8_score=8.0,
            has_docstrings=False,
            has_type_hints=False,
            test_coverage="basic",
            complexity_score=2.0,
        )

        review_response = CodeReviewResponse(
            code_quality_score=8.0,
            metrics=review_metrics,
            issues=[],
            recommendations=[],
            review_time=datetime.now(),
            tokens_used=150,
        )

        mock_generator.process.return_value = gen_response
        mock_reviewer.process.return_value = review_response

        orchestrator = MultiAgentOrchestrator(
            generator_agent=mock_generator,
            reviewer_agent=mock_reviewer,
        )

        request = OrchestratorRequest(
            task_description="Test task",
            language="python",
            model_name="starcoder",
        )

        # Execute
        response = await orchestrator.process_task(request)

        # Verify
        assert response.success is True
        assert response.generation_result is not None
        assert response.review_result is not None
        assert mock_generator.process.called
        assert mock_reviewer.process.called


class TestMultiModelComparisonWorkflow:
    """Test multi-model comparison workflow."""

    @pytest.mark.asyncio
    async def test_comparing_models_on_same_task(self):
        """Test comparing multiple models on the same task."""
        # Setup models with mock reviewers
        agents = {
            "starcoder": (
                AsyncMock(spec=CodeGeneratorAgent),
                AsyncMock(spec=CodeReviewerAgent),
            ),
            "mistral": (
                AsyncMock(spec=CodeGeneratorAgent),
                AsyncMock(spec=CodeReviewerAgent),
            ),
        }

        for model_name, (gen_agent, rev_agent) in agents.items():
            gen_agent.process.return_value = CodeGenerationResponse(
                task_description="Test",
                generated_code=f"# Generated by {model_name}\ndef test(): pass",
                tests=f"def test_{model_name}(): pass",
                metadata=TaskMetadata(),
                tokens_used=100,
            )

            rev_agent.process.return_value = CodeReviewResponse(
                code_quality_score=8.0,
                metrics=CodeQualityMetrics(
                    pep8_compliance=True,
                    pep8_score=8.0,
                    has_docstrings=True,
                    has_type_hints=True,
                    test_coverage="good",
                    complexity_score=2.0,
                ),
                issues=[],
                recommendations=[],
                tokens_used=150,
            )

        # Execute with different models
        results = {}
        for model_name, (gen_agent, rev_agent) in agents.items():
            orchestrator = MultiAgentOrchestrator(
                generator_agent=gen_agent, reviewer_agent=rev_agent
            )
            request = OrchestratorRequest(
                task_description="Test",
                model_name=model_name,
            )
            response = await orchestrator.process_task(request)
            results[model_name] = response.success

        # Verify
        assert all(results.values())


class TestTokenAnalysisToCompressionWorkflow:
    """Test token analysis to compression workflow."""

    @pytest.mark.asyncio
    async def test_token_analysis_triggers_compression(self):
        """Test that token analysis triggers compression when needed."""
        from src.domain.services.token_analyzer import TokenAnalyzer

        # Create long text that exceeds limits
        long_text = " ".join(["word"] * 5000)

        # Analyze and compress
        compressed_text, metadata = TokenAnalyzer.analyze_and_compress(
            long_text, model_name="starcoder"
        )

        # Verify
        assert metadata["compression_applied"] is True
        assert len(compressed_text) < len(long_text)
        assert "compression_ratio" in metadata


class TestRiddleTestingWorkflow:
    """Test riddle testing workflow."""

    @pytest.mark.asyncio
    async def test_riddle_evaluation_workflow(self):
        """Test riddle evaluation workflow."""
        from src.domain.services.riddle_evaluator import RiddleEvaluator

        evaluator = RiddleEvaluator()

        # Check if riddles are available
        assert evaluator.riddles is not None
        assert len(evaluator.riddles) > 0

        # Test with a sample riddle - checking the structure
        riddle = evaluator.riddles[0]

        # Check if it has the expected fields
        if "question" in riddle:
            question = riddle["question"]
            answer = riddle.get("answer", "some answer")

            analysis = evaluator.analyze_response(question, answer)
            assert analysis is not None
        else:
            # Alternative structure
            assert "text" in riddle or "riddle" in riddle


class TestOrchestratorWithRealAgents:
    """Test orchestrator with real agent implementations."""

    @pytest.mark.asyncio
    async def test_orchestrator_with_real_agents(self):
        """Test orchestrator initializes with real agents."""
        orchestrator = MultiAgentOrchestrator()

        assert orchestrator.generator is not None
        assert orchestrator.reviewer is not None
        assert isinstance(orchestrator.generator, CodeGeneratorAgent)
        assert isinstance(orchestrator.reviewer, CodeReviewerAgent)


class TestErrorPropagationThroughWorkflow:
    """Test error propagation through workflow."""

    @pytest.mark.asyncio
    async def test_generation_error_stops_workflow(self):
        """Test that generation error stops the workflow."""
        mock_generator = AsyncMock(spec=CodeGeneratorAgent)
        mock_generator.process.side_effect = Exception("Generation failed")

        orchestrator = MultiAgentOrchestrator(generator_agent=mock_generator)

        request = OrchestratorRequest(
            task_description="Test",
            model_name="starcoder",
        )

        response = await orchestrator.process_task(request)

        assert response.success is False
        assert response.error_message is not None

    @pytest.mark.asyncio
    async def test_review_error_handled_gracefully(self):
        """Test that review error is handled gracefully."""
        mock_generator = AsyncMock(spec=CodeGeneratorAgent)
        mock_reviewer = AsyncMock(spec=CodeReviewerAgent)

        mock_generator.process.return_value = CodeGenerationResponse(
            task_description="Test",
            generated_code="def test(): pass",
            tests="def test_test(): pass",
            metadata=TaskMetadata(),
            tokens_used=100,
        )

        mock_reviewer.process.side_effect = Exception("Review failed")

        orchestrator = MultiAgentOrchestrator(
            generator_agent=mock_generator,
            reviewer_agent=mock_reviewer,
        )

        request = OrchestratorRequest(
            task_description="Test",
            model_name="starcoder",
        )

        response = await orchestrator.process_task(request)

        assert response.success is False
        assert (
            "Review" in response.error_message
            or "error" in response.error_message.lower()
        )


class TestStateManagementAcrossWorkflow:
    """Test state management across workflow."""

    @pytest.mark.asyncio
    async def test_workflow_tracks_statistics(self):
        """Test that workflow tracks statistics correctly."""
        mock_generator = AsyncMock(spec=CodeGeneratorAgent)
        mock_reviewer = AsyncMock(spec=CodeReviewerAgent)

        mock_generator.process.return_value = CodeGenerationResponse(
            task_description="Test",
            generated_code="code",
            tests="tests",
            metadata=TaskMetadata(),
            tokens_used=100,
        )

        mock_reviewer.process.return_value = CodeReviewResponse(
            code_quality_score=8.0,
            metrics=CodeQualityMetrics(
                pep8_compliance=True,
                pep8_score=8.0,
                has_docstrings=True,
                has_type_hints=True,
                test_coverage="good",
                complexity_score=2.0,
            ),
            issues=[],
            recommendations=[],
            tokens_used=150,
        )

        orchestrator = MultiAgentOrchestrator(
            generator_agent=mock_generator,
            reviewer_agent=mock_reviewer,
        )

        request = OrchestratorRequest(
            task_description="Test",
            model_name="starcoder",
        )

        # Execute multiple workflows
        await orchestrator.process_task(request)
        await orchestrator.process_task(request)

        stats = orchestrator.get_stats()
        assert stats["total_workflows"] == 2


class TestConcurrentAgentExecution:
    """Test concurrent agent execution."""

    @pytest.mark.asyncio
    async def test_concurrent_agent_execution(self):
        """Test executing multiple agents concurrently."""
        from src.application.orchestrators.parallel_orchestrator import (
            ParallelOrchestrator,
        )

        orchestrator = ParallelOrchestrator()

        # Create multiple mock agents
        tasks = []
        for i in range(3):
            agent = AsyncMock()
            agent.process.return_value = f"result_{i}"
            tasks.append((agent, None))

        results = await orchestrator.execute_parallel(tasks, fail_fast=False)

        assert len(results) == 3
        assert "result_0" in results
        assert "result_1" in results
        assert "result_2" in results
