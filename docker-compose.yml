version: '3.8'

services:
  llm-server:
    image: vllm/vllm-openai:latest
    container_name: mistral-vllm
    ports:
      - "8080:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN}
    command: ["--model=mistralai/Mistral-7B-Instruct-v0.2", "--tensor-parallel-size=1", "--dtype=float16", "--max-model-len=32768"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  app:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    environment:
      - LLM_URL=http://llm-server:8000
      - LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
      - SUMMARIZER_LANGUAGE=ru
    depends_on:
      llm-server:
        condition: service_healthy
    command: python -c "print('app ready')"

version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-challenge-api
    ports:
      - "8000:8000"
    environment:
      - STORAGE_PATH=/app/data
      - MODEL_NAME=gpt-4
      - MODEL_MAX_TOKENS=4096
      - MODEL_TEMPERATURE=0.7
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - ai-challenge-network

networks:
  ai-challenge-network:
    driver: bridge

