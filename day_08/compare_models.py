"""
Model comparison script for testing multiple models.

This script compares StarCoder, Mistral, and Qwen models
with automatic Docker container management.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add shared package to path
shared_path = Path(__file__).parent.parent / "shared"
sys.path.insert(0, str(shared_path))

from shared_package.clients.unified_client import UnifiedModelClient
from core.token_analyzer import TokenCounter, LimitProfile
from core.text_compressor import SimpleTextCompressor
from core.experiments import TokenLimitExperiments
from utils.console_reporter import ConsoleReporter


async def main():
    """Compare StarCoder, Mistral, and Qwen models."""
    
    print("üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: StarCoder vs Mistral vs Qwen")
    print("="*60)
    
    # Configuration
    token_mode = os.getenv("TOKEN_COUNTER_MODE", "accurate")
    limit_profile = LimitProfile(os.getenv("LIMIT_PROFILE", "practical"))
    auto_swap = os.getenv("AUTO_SWAP_MODELS", "true").lower() == "true"
    
    print(f"\nüîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   –†–µ–∂–∏–º –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤: {token_mode}")
    print(f"   –ü—Ä–æ—Ñ–∏–ª—å –ª–∏–º–∏—Ç–æ–≤: {limit_profile.value}")
    print(f"   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏: {auto_swap}")
    
    # Initialize components
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
    token_counter = TokenCounter(mode=token_mode, limit_profile=limit_profile)
    text_compressor = SimpleTextCompressor(token_counter)
    model_client = UnifiedModelClient()
    reporter = ConsoleReporter()
    
    # Check Docker manager availability
    docker_manager = None
    if auto_swap:
        try:
            from utils.docker_manager import ModelDockerManager
            docker_manager = ModelDockerManager()
            print("‚úÖ Docker –º–µ–Ω–µ–¥–∂–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except ImportError:
            print("‚ö†Ô∏è  Docker –º–µ–Ω–µ–¥–∂–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –æ—Ç–∫–ª—é—á–∞–µ–º auto-swap")
            auto_swap = False
    
    print("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    # Models to compare
    models = ["starcoder", "mistral", "qwen"]
    
    # Test queries
    queries = [
        "–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
        "–ù–∞–ø–∏—à–∏ –ø—Ä–æ—Å—Ç—É—é —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å–ø–∏—Å–∫–∞"
    ]
    
    print(f"\nüìù –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:")
    for i, query in enumerate(queries, 1):
        print(f"   {i}. {query}")
    
    # Create experimenter
    experimenter = TokenLimitExperiments(model_client, token_counter, text_compressor)
    
    all_results = []
    
    # Run experiments for each query
    for query_idx, query in enumerate(queries, 1):
        print(f"\n{'='*60}")
        print(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ {query_idx}/{len(queries)}")
        print(f"üìù –ó–∞–ø—Ä–æ—Å: {query}")
        print(f"{'='*60}")
        
        # Check which models are available
        available_models = []
        for model in models:
            is_available = await model_client.check_availability(model)
            if is_available:
                available_models.append(model)
                print(f"‚úÖ {model} –¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                print(f"‚ùå {model} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        if not available_models:
            print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            continue
        
        # Run comparison experiment
        try:
            results = await experimenter.run_model_comparison_experiment(
                models=available_models,
                query=query,
                auto_swap=auto_swap
            )
            
            # Add query info to results
            for result in results:
                result.experiment_name = f"query_{query_idx}_{result.experiment_name}"
            
            all_results.extend(results)
            
            print(f"‚úÖ –ó–∞–ø—Ä–æ—Å {query_idx} –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω —Å {len(results)} –º–æ–¥–µ–ª—è–º–∏")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ {query_idx}: {e}")
    
    # Generate comprehensive reports
    if all_results:
        print(f"\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –¥–ª—è {len(all_results)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")
        
        reporter.print_experiment_summary(all_results)
        reporter.print_detailed_analysis(all_results)
        reporter.print_recommendations(all_results)
        reporter.print_model_performance(all_results)
        reporter.print_final_statistics(all_results)
        
        # Model-specific analysis
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –ø–æ –º–æ–¥–µ–ª—è–º:")
        for model in models:
            model_results = [r for r in all_results if r.model_name == model]
            if model_results:
                avg_time = sum(r.response_time for r in model_results) / len(model_results)
                avg_tokens = sum(r.total_tokens for r in model_results) / len(model_results)
                print(f"   {model}: {len(model_results)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, "
                      f"—Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}—Å, —Å—Ä–µ–¥–Ω–∏–µ —Ç–æ–∫–µ–Ω—ã: {avg_tokens:.0f}")
        
        print("‚úÖ –í—Å–µ –æ—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!")
    else:
        print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    print("\nüéâ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    asyncio.run(main())
