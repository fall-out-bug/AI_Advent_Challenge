"""
Unified entry point for the token analysis system.

This module provides a single entry point that replaces multiple
main files (main.py, main_enhanced.py, demo.py) with a clean,
configurable interface.
"""

import asyncio
import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any

# Add shared package to path
shared_path = Path(__file__).parent.parent / "shared"
sys.path.insert(0, str(shared_path))

from core.bootstrap import ApplicationBootstrapper, BootstrapError
from models.application_context import ApplicationContext
from utils.logging import LoggerFactory


async def run_experiments(context: ApplicationContext) -> None:
    """
    Run full token limit experiments.
    
    Args:
        context: Application context with all components
    """
    context.logger.info("Starting full token limit experiments")
    
    try:
        # Check StarCoder availability
        context.logger.info("Checking StarCoder availability")
        is_available = await context.ml_client.check_availability("starcoder")
        
        if not is_available:
            context.logger.error("StarCoder is not available")
            print("âŒ StarCoder Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½!")
            print("ðŸ’¡ Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ: cd ../local_models && docker-compose up -d starcoder-chat")
            return
        
        context.logger.info("StarCoder is available")
        print("âœ… StarCoder Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½!")
        
        # Run experiments
        context.logger.info("Running limit exceeded experiments")
        print("\nðŸ§ª Ð—Ð°Ð¿ÑƒÑÐº ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¸ÐµÐ¼ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð²...")
        
        results = await context.experiments.run_limit_exceeded_experiment("starcoder")
        
        if not results:
            context.logger.error("No experiment results received")
            print("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
            return
        
        context.logger.info(f"Received {len(results)} experiment results")
        print(f"âœ… ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {len(results)} Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
        
        # Generate reports
        await _generate_reports(context, results)
        
        # Show summary
        await _show_experiment_summary(context, results)
        
        context.logger.info("Experiments completed successfully")
        print("\nðŸŽ‰ Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ñ‹ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!")
        
    except Exception as e:
        context.logger.error(f"Error during experiments: {e}")
        print(f"\nâŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²: {e}")
        raise


async def run_demo(context: ApplicationContext) -> None:
    """
    Run demo with short queries for quick testing.
    
    Args:
        context: Application context with all components
    """
    context.logger.info("Starting demo with short queries")
    
    try:
        # Check availability
        is_available = await context.ml_client.check_availability("starcoder")
        if not is_available:
            print("âŒ StarCoder Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½!")
            return
        
        # Run short query experiments
        context.logger.info("Running short query experiments")
        results = await context.experiments.run_short_query_experiment("starcoder")
        
        # Generate basic reports
        context.reporter.print_experiment_summary(results)
        context.reporter.print_detailed_analysis(results)
        context.reporter.print_recommendations(results)
        
        context.logger.info("Demo completed successfully")
        print("\nâœ… Ð”ÐµÐ¼Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        
    except Exception as e:
        context.logger.error(f"Error during demo: {e}")
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð´ÐµÐ¼Ð¾: {e}")
        raise


async def run_comparison(context: ApplicationContext) -> None:
    """
    Run model comparison experiments.
    
    Args:
        context: Application context with all components
    """
    context.logger.info("Starting model comparison experiments")
    
    try:
        # Check availability
        is_available = await context.ml_client.check_availability("starcoder")
        if not is_available:
            print("âŒ StarCoder Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½!")
            return
        
        # Run model comparison
        context.logger.info("Running model comparison")
        models = ["starcoder", "mistral", "qwen"]
        query = "ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ð° Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð² Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð°Ñ…"
        
        results = await context.experiments.run_model_comparison_experiment(
            models=models,
            query=query,
            auto_swap=True
        )
        
        # Generate reports
        await _generate_reports(context, results)
        
        context.logger.info("Model comparison completed successfully")
        print("\nðŸŽ‰ Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!")
        
    except Exception as e:
        context.logger.error(f"Error during model comparison: {e}")
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: {e}")
        raise


async def run_advanced_compression(context: ApplicationContext) -> None:
    """
    Run advanced compression experiments.
    
    Args:
        context: Application context with all components
    """
    context.logger.info("Starting advanced compression experiments")
    
    try:
        # Run advanced compression experiments
        context.logger.info("Running advanced compression experiments")
        strategies = ["truncation", "keywords", "extractive", "semantic"]
        
        results = await context.experiments.run_advanced_compression_experiment(
            model_name="starcoder",
            strategies=strategies
        )
        
        # Generate reports
        await _generate_reports(context, results)
        
        context.logger.info("Advanced compression experiments completed successfully")
        print("\nðŸŽ‰ ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ñ‹!")
        
    except Exception as e:
        context.logger.error(f"Error during advanced compression: {e}")
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ñ… ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ñ…: {e}")
        raise


async def _generate_reports(context: ApplicationContext, results: list) -> None:
    """
    Generate all reports for experiment results.
    
    Args:
        context: Application context
        results: Experiment results
    """
    context.logger.info("Generating reports")
    print("\nðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²...")
    
    context.reporter.print_experiment_summary(results)
    context.reporter.print_detailed_analysis(results)
    context.reporter.print_recommendations(results)
    context.reporter.print_compression_comparison(results)
    context.reporter.print_model_performance(results)
    
    context.logger.info("All reports generated")
    print("âœ… Ð’ÑÐµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹!")


async def _show_experiment_summary(context: ApplicationContext, results: list) -> None:
    """
    Show experiment summary statistics.
    
    Args:
        context: Application context
        results: Experiment results
    """
    context.logger.info("Showing experiment summary")
    
    summary = context.experiments.get_experiment_summary(results)
    print(f"\nðŸ“ˆ Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:")
    print(f"   Ð’ÑÐµÐ³Ð¾ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²: {summary['total_experiments']}")
    print(f"   Ð£ÑÐ¿ÐµÑˆÐ½Ñ‹Ñ…: {summary['successful_experiments']}")
    print(f"   ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ ÑƒÑÐ¿ÐµÑ…Ð°: {summary['success_rate']*100:.1f}%")
    print(f"   Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {summary['avg_response_time']:.2f} ÑÐµÐº")
    print(f"   ÐžÐ±Ñ‰Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹: {summary['total_tokens_used']}")


def load_config() -> Dict[str, Any]:
    """
    Load configuration from environment variables.
    
    Returns:
        Dict[str, Any]: Configuration dictionary
    """
    config = {
        'token_counter_mode': os.getenv("TOKEN_COUNTER_MODE", "simple"),
        'limit_profile': os.getenv("LIMIT_PROFILE", "practical"),
        'ml_service_url': os.getenv("ML_SERVICE_URL", "http://localhost:8004"),
        'log_level': os.getenv("LOG_LEVEL", "INFO"),
        'debug': os.getenv("DEBUG", "false").lower() == "true"
    }
    
    return config


def print_help() -> None:
    """Print help information."""
    print("""
ðŸš€ Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² - Day 8

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ:
    python run.py                    # Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð¾Ð»Ð½Ñ‹Ñ… ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¾Ð²
    python run.py --demo             # Ð—Ð°Ð¿ÑƒÑÐº Ð´ÐµÐ¼Ð¾ Ñ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¼Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
    python run.py --comparison       # Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
    python run.py --advanced         # ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ ÑÐ¶Ð°Ñ‚Ð¸Ñ
    python run.py --help             # ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÑ‚Ñƒ ÑÐ¿Ñ€Ð°Ð²ÐºÑƒ

ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ:
    TOKEN_COUNTER_MODE=simple        # Ð ÐµÐ¶Ð¸Ð¼ Ð¿Ð¾Ð´ÑÑ‡ÐµÑ‚Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
    LIMIT_PROFILE=practical          # ÐŸÑ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð² (theoretical/practical)
    ML_SERVICE_URL=http://localhost:8004  # URL ML ÑÐµÑ€Ð²Ð¸ÑÐ°
    LOG_LEVEL=INFO                   # Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    DEBUG=false                      # Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸

Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ:
    - StarCoder Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½ (Ð¿Ð¾Ñ€Ñ‚ 8003)
    - Ð—Ð°Ð¿ÑƒÑÐº: cd ../local_models && docker-compose up -d starcoder-chat

Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹:
    1. Ð‘ÐµÐ· ÑÐ¶Ð°Ñ‚Ð¸Ñ - Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð´Ð»Ð¸Ð½Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° ÐºÐ°Ðº ÐµÑÑ‚ÑŒ
    2. Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ñ€ÐµÐ·ÐºÑƒ - ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸ ÐºÐ¾Ð½Ñ†Ð°
    3. Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° - Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð²

Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹:
    - ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ Ð²ÑÐµÑ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²
    - ÐÐ½Ð°Ð»Ð¸Ð· ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ ÑÐ¶Ð°Ñ‚Ð¸Ñ
    - Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    """)


async def main() -> None:
    """
    Main function that orchestrates the token analysis system.
    
    Initializes all components using ApplicationBootstrapper,
    then runs the selected experiment type.
    """
    print("ðŸš€ Ð—Ð°Ð¿ÑƒÑÐº ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²")
    print("="*50)
    
    try:
        # Load configuration
        config = load_config()
        
        # Initialize application
        bootstrapper = ApplicationBootstrapper(config)
        context = bootstrapper.bootstrap()
        
        print("âœ… ÐŸÑ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾")
        print(f"ðŸ“‹ ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ: {config}")
        
        # Determine experiment type
        experiment_type = _get_experiment_type()
        
        # Run selected experiment
        await _run_experiment(context, experiment_type)
        
    except BootstrapError as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ñ€ÐµÑ€Ð²Ð°Ð½Ñ‹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼")
    except Exception as e:
        print(f"\nâŒ ÐÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
        raise
    finally:
        # Cleanup
        if 'context' in locals():
            await context.cleanup()


def _get_experiment_type() -> str:
    """
    Get experiment type from command line arguments.
    
    Returns:
        str: Experiment type
    """
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        if arg == "--demo":
            return "demo"
        elif arg == "--comparison":
            return "comparison"
        elif arg == "--advanced":
            return "advanced"
        elif arg == "--help":
            print_help()
            sys.exit(0)
        else:
            print("âŒ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ --help Ð´Ð»Ñ ÑÐ¿Ñ€Ð°Ð²ÐºÐ¸")
            sys.exit(1)
    
    return "full"


async def _run_experiment(context: ApplicationContext, experiment_type: str) -> None:
    """
    Run the specified experiment type.
    
    Args:
        context: Application context
        experiment_type: Type of experiment to run
    """
    if experiment_type == "demo":
        await run_demo(context)
    elif experiment_type == "comparison":
        await run_comparison(context)
    elif experiment_type == "advanced":
        await run_advanced_compression(context)
    else:  # full
        await run_experiments(context)


if __name__ == "__main__":
    asyncio.run(main())
