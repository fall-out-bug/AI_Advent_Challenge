"""
Enhanced main entry point for the token analysis system.

This module provides the main function that orchestrates all components
with ML service integration and hybrid token counting.
"""

import asyncio
import sys
import os
from pathlib import Path

# Add shared package to path
shared_path = Path(__file__).parent.parent / "shared"
sys.path.insert(0, str(shared_path))

from shared_package.clients.unified_client import UnifiedModelClient
from core.token_analyzer import TokenCounter, LimitProfile
from core.text_compressor import SimpleTextCompressor, AdvancedTextCompressor
from core.ml_client import TokenAnalysisClient, HybridTokenCounter
from core.experiments import TokenLimitExperiments
from utils.console_reporter import ConsoleReporter


async def main():
    """
    Enhanced main function with ML service integration.
    
    Supports configuration via environment variables and provides
    hybrid token counting with ML service fallback.
    """
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫–µ–Ω–æ–≤")
    print("="*60)
    
    # Read configuration from environment
    token_mode = os.getenv("TOKEN_COUNTER_MODE", "hybrid")
    limit_profile_str = os.getenv("LIMIT_PROFILE", "practical")
    
    try:
        limit_profile = LimitProfile(limit_profile_str)
    except ValueError:
        print(f"‚ö†Ô∏è  –ù–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –ª–∏–º–∏—Ç–æ–≤: {limit_profile_str}, –∏—Å–ø–æ–ª—å–∑—É–µ–º practical")
        limit_profile = LimitProfile.PRACTICAL
    
    print(f"\nüîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   –†–µ–∂–∏–º –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤: {token_mode}")
    print(f"   –ü—Ä–æ—Ñ–∏–ª—å –ª–∏–º–∏—Ç–æ–≤: {limit_profile.value}")
    
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
    
    # Initialize ML service client
    ml_client = TokenAnalysisClient()
    
    # Initialize fallback counter
    fallback_counter = TokenCounter(mode="simple", limit_profile=limit_profile)
    
    # Initialize hybrid counter
    token_counter = HybridTokenCounter(ml_client, fallback_counter)
    
    # Check ML service availability
    try:
        health = await ml_client.health_check()
        print(f"‚úÖ ML —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω! –ú–æ–¥–µ–ª–∏: {health.get('available_models', [])}")
        ml_service_available = True
    except Exception as e:
        print(f"‚ö†Ô∏è  ML —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ ML —Å–µ—Ä–≤–∏—Å: make docker-run")
        ml_service_available = False
    
    # Initialize text compressor
    if ml_service_available and token_mode == "hybrid":
        # Use advanced compressor with ML service
        text_compressor = AdvancedTextCompressor(token_counter, None)  # No model_client for now
    else:
        # Use simple compressor
        text_compressor = SimpleTextCompressor(token_counter)
    
    model_client = UnifiedModelClient()
    reporter = ConsoleReporter()
    print("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ StarCoder
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ StarCoder...")
    is_available = await model_client.check_availability("starcoder")
    
    if not is_available:
        print("‚ùå StarCoder –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: cd ../local_models && docker-compose up -d starcoder-chat")
        return
    
    print("‚úÖ StarCoder –¥–æ—Å—Ç—É–ø–µ–Ω!")
    
    print("\nüß™ –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞...")
    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞
    experimenter = TokenLimitExperiments(model_client, token_counter, text_compressor)
    print("‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω")
    
    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    print("\nüéØ –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:")
    print("1. –ë–∞–∑–æ–≤—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ–º –ª–∏–º–∏—Ç–æ–≤")
    print("2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–∂–∞—Ç–∏—è")
    print("3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (StarCoder, Mistral, Qwen)")
    print("4. –¢–µ—Å—Ç —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤")
    
    choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-4) –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –±–∞–∑–æ–≤—ã—Ö: ").strip()
    
    try:
        if choice == "2":
            # Advanced compression experiments
            print("\nüß™ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ —Å–∂–∞—Ç–∏—è...")
            strategies = ["truncation", "keywords", "extractive", "semantic"]
            results = await experimenter.run_advanced_compression_experiment(
                model_name="starcoder",
                strategies=strategies
            )
        elif choice == "3":
            # Model comparison
            print("\nüß™ –ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")
            models = ["starcoder", "mistral", "qwen"]
            query = "–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö"
            results = await experimenter.run_model_comparison_experiment(
                models=models,
                query=query,
                auto_swap=True
            )
        elif choice == "4":
            # Token counting test
            print("\nüß™ –¢–µ—Å—Ç —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤...")
            test_texts = [
                "Hello world",
                "This is a longer text with multiple words to test token counting accuracy",
                "–î–µ—Ç–∞–ª—å–Ω–æ –æ–±—ä—è—Å–Ω–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏"
            ]
            
            print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤:")
            for text in test_texts:
                print(f"\n–¢–µ–∫—Å—Ç: {text[:50]}...")
                
                # Simple estimation
                simple_result = fallback_counter.count_tokens(text, "starcoder")
                print(f"   –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: {simple_result.count} —Ç–æ–∫–µ–Ω–æ–≤")
                
                # Accurate counting (if ML service available)
                if ml_service_available:
                    try:
                        accurate_result = await token_counter.count_tokens(text, "starcoder")
                        print(f"   –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç: {accurate_result.count} —Ç–æ–∫–µ–Ω–æ–≤")
                        print(f"   –†–∞–∑–Ω–∏—Ü–∞: {abs(accurate_result.count - simple_result.count)} —Ç–æ–∫–µ–Ω–æ–≤")
                    except Exception as e:
                        print(f"   –û—à–∏–±–∫–∞ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞: {e}")
                else:
                    print("   –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (ML —Å–µ—Ä–≤–∏—Å –Ω–µ –∑–∞–ø—É—â–µ–Ω)")
            
            print("\n‚úÖ –¢–µ—Å—Ç –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            return
        else:
            # Basic experiments (default)
            print("\nüß™ –ó–∞–ø—É—Å–∫ –±–∞–∑–æ–≤—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ–º –ª–∏–º–∏—Ç–æ–≤...")
            results = await experimenter.run_limit_exceeded_experiment("starcoder")
        
        if not results:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
            return
        
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
        print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        reporter.print_experiment_summary(results)
        reporter.print_detailed_analysis(results)
        reporter.print_recommendations(results)
        reporter.print_model_performance(results)
        reporter.print_final_statistics(results)
        print("‚úÖ –í—Å–µ –æ—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!")
        
        print("\nüéâ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–µ—Ä–≤–∞–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Cleanup
        await token_counter.close()
        await model_client.close()


if __name__ == "__main__":
    asyncio.run(main())
