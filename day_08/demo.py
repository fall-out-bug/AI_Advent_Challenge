"""
Demo script for the token analysis system.

This module provides simple usage examples and demonstrations
of the token counting and compression functionality.
"""

import asyncio
import sys
from pathlib import Path

# Add shared package to path
shared_path = Path(__file__).parent.parent / "shared"
sys.path.insert(0, str(shared_path))

from shared_package.clients.unified_client import UnifiedModelClient

from core.text_compressor import SimpleTextCompressor
from core.token_analyzer import SimpleTokenCounter
from utils.console_reporter import ConsoleReporter


async def demo_token_counting():
    """
    Demonstrate token counting functionality.
    """
    print("üî¢ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤")
    print("=" * 40)

    token_counter = SimpleTokenCounter()

    # Test different texts
    test_texts = [
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
        "–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.",
        "–î–µ—Ç–∞–ª—å–Ω–æ –æ–±—ä—è—Å–Ω–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, multi-head attention, positional encoding, feed-forward networks, layer normalization, residual connections, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è, –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.",
    ]

    for i, text in enumerate(test_texts, 1):
        token_info = token_counter.count_tokens(text)
        print(f"\nüìù –¢–µ–∫—Å—Ç {i}: {text[:50]}...")
        print(f"   –¢–æ–∫–µ–Ω–æ–≤: {token_info.count}")
        print(f"   –°–ª–æ–≤: {len(text.split())}")
        print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {token_info.count / len(text.split()):.2f}")

        # Check limits
        for model_name in ["starcoder", "tinyllama"]:
            limits = token_counter.get_model_limits(model_name)
            exceeds = token_counter.check_limit_exceeded(text, model_name)
            print(
                f"   {model_name}: {'‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω' if exceeds else '‚úÖ –í –ø—Ä–µ–¥–µ–ª–∞—Ö'} (–ª–∏–º–∏—Ç: {limits.max_input_tokens})"
            )


async def demo_text_compression():
    """
    Demonstrate text compression functionality.
    """
    print("\nüóúÔ∏è  –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–∞")
    print("=" * 40)

    token_counter = SimpleTokenCounter()
    text_compressor = SimpleTextCompressor(token_counter)

    # Long text that exceeds limits
    long_text = (
        """
    –î–µ—Ç–∞–ª—å–Ω–æ –æ–±—ä—è—Å–Ω–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. 
    –í–∫–ª—é—á–∏ –≤ –æ—Ç–≤–µ—Ç —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:
    
    1. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (attention mechanism):
    - –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç scaled dot-product attention
    - –§–æ—Ä–º—É–ª–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è attention weights
    - –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    - –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö
    
    2. Multi-head attention:
    - –ó–∞—á–µ–º –Ω—É–∂–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
    - –ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ single-head attention
    - –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤
    
    3. Positional encoding:
    - –ö–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤
    - Sinusoidal –∏ learned positional encoding
    - –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏
    - –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é
    
    4. Feed-forward networks:
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ FFN —Å–ª–æ–µ–≤
    - –†–∞–∑–º–µ—Ä—ã —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
    - –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
    - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    
    5. Layer normalization:
    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –µ—ë —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏
    - Pre-norm vs Post-norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    - –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å batch normalization
    
    6. Residual connections:
    - –ó–∞—á–µ–º –Ω—É–∂–Ω—ã skip connections
    - –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã vanishing gradients
    - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã residual connections
    - –í–ª–∏—è–Ω–∏–µ –Ω–∞ –≥–ª—É–±–∏–Ω—É —Å–µ—Ç–∏
    
    7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
    - –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
    - –ü—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (BERT, GPT, T5)
    - –û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è (NLP, Computer Vision, etc.)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏
    
    8. –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python:
    - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è attention mechanism —Å –Ω—É–ª—è
    - Multi-head attention –Ω–∞ PyTorch
    - –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
    - –û–±—É—á–µ–Ω–∏–µ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    
    9. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - –ö–∞–∫ —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
    - –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –∏ —Å–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–µ–π
    - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ attention –º–µ—Ö–∞–Ω–∏–∑–º—ã
    - Hardware –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    
    10. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:
    - BERT –∏ –µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
    - GPT —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π
    - T5 –∏ text-to-text –ø–æ–¥—Ö–æ–¥
    - Vision Transformers (ViT)
    - Efficient Transformers
    
    –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ñ–æ—Ä–º—É–ª–∞–º–∏.
    """
        * 2
    )  # Make it even longer

    print(f"üìù –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(long_text)} —Å–∏–º–≤–æ–ª–æ–≤")

    original_tokens = token_counter.count_tokens(long_text).count
    print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {original_tokens}")

    # Test compression strategies
    target_tokens = 1000  # Target for compression

    print(f"\nüéØ –¶–µ–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã: {target_tokens}")

    # Truncation compression
    print(f"\n‚úÇÔ∏è  –°–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –æ–±—Ä–µ–∑–∫—É:")
    truncation_result = text_compressor.compress_by_truncation(long_text, target_tokens)
    print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {truncation_result.compression_ratio:.2f}")
    print(f"   –°–∂–∞—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã: {truncation_result.compressed_tokens}")
    print(f"   –ü—Ä–µ–≤—å—é: {truncation_result.compressed_text[:200]}...")

    # Keywords compression
    print(f"\nüîë –°–∂–∞—Ç–∏–µ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
    keywords_result = text_compressor.compress_by_keywords(long_text, target_tokens)
    print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {keywords_result.compression_ratio:.2f}")
    print(f"   –°–∂–∞—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã: {keywords_result.compressed_tokens}")
    print(f"   –ü—Ä–µ–≤—å—é: {keywords_result.compressed_text[:200]}...")

    # Compare strategies
    print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:")
    print(
        f"   –û–±—Ä–µ–∑–∫–∞: {truncation_result.compression_ratio:.2f} (–ª—É—á—à–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç)"
    )
    print(
        f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords_result.compression_ratio:.2f} (–ª—É—á—à–µ –¥–ª—è –ø–æ–∏—Å–∫–∞)"
    )


async def demo_model_interaction():
    """
    Demonstrate interaction with StarCoder model.
    """
    print("\nü§ñ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥–µ–ª—å—é")
    print("=" * 40)

    try:
        model_client = UnifiedModelClient()
        token_counter = SimpleTokenCounter()

        # Check availability
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ StarCoder...")
        is_available = await model_client.check_availability("starcoder")

        if not is_available:
            print("‚ùå StarCoder –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
            print(
                "üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: cd ../local_models && docker-compose up -d starcoder-chat"
            )
            return

        print("‚úÖ StarCoder –¥–æ—Å—Ç—É–ø–µ–Ω!")

        # Test with short query
        short_query = "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏."
        print(f"\nüìù –ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å: {short_query}")

        input_tokens = token_counter.count_tokens(short_query).count
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {input_tokens}")

        response = await model_client.make_request(
            model_name="starcoder", prompt=short_query, max_tokens=100, temperature=0.7
        )

        output_tokens = token_counter.count_tokens(response.response).count
        print(f"üìä –í—ã—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {output_tokens}")
        print(f"üìä –û–±—â–∏–µ —Ç–æ–∫–µ–Ω—ã: {input_tokens + output_tokens}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response.response_time:.2f} —Å–µ–∫")
        print(f"üìù –û—Ç–≤–µ—Ç: {response.response}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –º–æ–¥–µ–ª—å—é: {e}")


async def demo_full_workflow():
    """
    Demonstrate the full workflow with compression.
    """
    print("\nüîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞")
    print("=" * 40)

    try:
        model_client = UnifiedModelClient()
        token_counter = SimpleTokenCounter()
        text_compressor = SimpleTextCompressor(token_counter)

        # Check availability
        is_available = await model_client.check_availability("starcoder")
        if not is_available:
            print("‚ùå StarCoder –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
            return

        # Create a query that exceeds limits
        long_query = (
            """
        –î–µ—Ç–∞–ª—å–Ω–æ –æ–±—ä—è—Å–Ω–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, 
        –≤–∫–ª—é—á–∞—è –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, multi-head attention, positional encoding, 
        feed-forward networks, layer normalization, residual connections, 
        –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è, –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –Ω–∞ Python, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ 
        —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç.
        """
            * 5
        )  # Make it exceed limits

        print(f"üìù –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {len(long_query)} —Å–∏–º–≤–æ–ª–æ–≤")

        original_tokens = token_counter.count_tokens(long_query).count
        limits = token_counter.get_model_limits("starcoder")

        print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {original_tokens}")
        print(f"üìä –õ–∏–º–∏—Ç –º–æ–¥–µ–ª–∏: {limits.max_input_tokens}")

        if original_tokens > limits.max_input_tokens:
            print("‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞! –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ...")

            # Compress using truncation
            compression_result = text_compressor.compress_by_truncation(
                long_query, limits.max_input_tokens
            )

            print(f"üóúÔ∏è  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {compression_result.compression_ratio:.2f}")
            print(f"üìä –°–∂–∞—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã: {compression_result.compressed_tokens}")

            # Send compressed query
            response = await model_client.make_request(
                model_name="starcoder",
                prompt=compression_result.compressed_text,
                max_tokens=500,
                temperature=0.7,
            )

            output_tokens = token_counter.count_tokens(response.response).count
            print(f"üìä –í—ã—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {output_tokens}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response.response_time:.2f} —Å–µ–∫")
            print(f"üìù –û—Ç–≤–µ—Ç: {response.response[:200]}...")

        else:
            print("‚úÖ –ó–∞–ø—Ä–æ—Å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞, —Å–∂–∞—Ç–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ–ª–Ω–æ–º —Ä–∞–±–æ—á–µ–º –ø—Ä–æ—Ü–µ—Å—Å–µ: {e}")


async def main():
    """
    Main demo function that runs all demonstrations.
    """
    print("üé¨ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫–µ–Ω–æ–≤")
    print("=" * 50)

    try:
        # Run all demos
        await demo_token_counting()
        await demo_text_compression()
        await demo_model_interaction()
        await demo_full_workflow()

        print("\n‚úÖ –í—Å–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
        print("\nüí° –î–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python main.py")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")


if __name__ == "__main__":
    asyncio.run(main())
